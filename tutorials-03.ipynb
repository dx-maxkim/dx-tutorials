{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbf2d92-815e-4a2e-b468-26f71cd8dbe3",
   "metadata": {},
   "source": [
    "# DEEPX Tutorial 03 - AI Porject Workflow with DEEPX NPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3b426-7d6e-4844-bf75-737a94d1055f",
   "metadata": {},
   "source": [
    "This third tutorial demonstrates the full end-to-end workflow for AI model deployment on DEEPX hardware.\n",
    "\n",
    "We will train a forklift and worker detection model, utilize the DX-COM tool for conversion to the DXNN format, and execute the final AI application on a DEEPX NPU. This process will provide a comprehensive picture of the DEEPX NPU development pipeline. \n",
    "\n",
    "This tutorial is based on dx-all-suite v2.0.0, released in September 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ad29e-2955-4f2e-beac-2e2a34bbe81f",
   "metadata": {},
   "source": [
    "## Hands-on Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c1d96-a612-4246-af9c-ebf798355fba",
   "metadata": {},
   "source": [
    "- **Detection classes**: Forklift, Worker\n",
    "- **Base AI model**: YOLOv7\n",
    "- **Dataset**: 1448 images about Forklift & Worker from [kaggle](https://www.kaggle.com/datasets/hakantaskiner/personforklift-dataset/data)\n",
    "- **Train**: Need NVIDIA GPU with more than 24G GRAM\n",
    "- **Inference NPU**: `DX-M1`\n",
    "- **AI application**: Modify and reuse the yolo demo of the DX-APP\n",
    "- **Expected output**:\n",
    "  ![img](assets/detection-goal.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8217ad7-fc19-421b-967d-98ff2dfbc762",
   "metadata": {},
   "source": [
    "## AI Workflow Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d81f9-0699-45e6-ba37-e27a481ad370",
   "metadata": {},
   "source": [
    "This diagram explains the common workflow of an AI project.\n",
    "\n",
    "We define the goal, collect and label data, and train the model.\n",
    "DX-Compiler helps make the model faster and lighter (INT8) for DX NPU.\n",
    "The final step is deploying the model to the DEEPX NPU using DX-App or DX-Stream.\n",
    "\n",
    "Each step builds toward real-world AI solutions, such as worker and forklift detection.\n",
    "![img](assets/workflow2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e8f9d-0115-48d7-a02a-1b64c84c3b72",
   "metadata": {},
   "source": [
    "## 1. AI Workflow - Model Selection based on the use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa66140-dbee-4bf2-a9ce-82f4316a8512",
   "metadata": {},
   "source": [
    "To start an AI project, we need to select an AI model that fits the use case.\n",
    "\n",
    "In this tutorial, our goal is to detect forklifts and workers.\n",
    "We will use YOLOv7, a well-known model for object detection.\n",
    "\n",
    "- Choose YOLOv7 to detect Forklift & Worker\n",
    "- For more details of YOLOv7: ðŸ‘‰ [link](https://docs.ultralytics.com/models/yolov7/)\n",
    "- How to use YOLOv7: ðŸ‘‰ [link](https://github.com/WongKinYiu/yolov7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81b7c5-ec05-412b-bc79-338f77d02b18",
   "metadata": {},
   "source": [
    "## 2. AI Workflow - Data Preparation & Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53d3cc-8142-4d59-ac44-fb7f2f836e74",
   "metadata": {},
   "source": [
    "Download the forklift-person labeled dataset from Kaggle:\n",
    " - Reference: [kaggle link](https://www.kaggle.com/datasets/hakantaskiner/personforklift-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071e952-82bb-4cf9-8c88-6a007b150d3b",
   "metadata": {},
   "source": [
    "## 3. AI Workflow - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd31d3c-e849-49d3-b7f4-10efefca0861",
   "metadata": {},
   "source": [
    "To train the model efficiently, you should use a GPU that has 24GB or more graphic memory.\n",
    "\n",
    " - How to train YOLOv7: ðŸ‘‰ [Link](https://colab.research.google.com/drive/1dAdjJuhXqFM_Qcd0QqAn7_AGx7abA5aX?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd943a-796f-4aec-8341-a524c71c4e2f",
   "metadata": {},
   "source": [
    "## 4. AI Workflow - Optimization with DX-Compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347d2b0-3fd8-4a96-bc0e-84c5885586eb",
   "metadata": {},
   "source": [
    "Let's try to compile your pre-trained AI model to DXNN format.\n",
    "\n",
    "Overall processes are:\n",
    "1. Get a pre-trained mode on pytorch framework\n",
    "2. Convert it to ONNX format\n",
    "3. Compile ONNX to DXNN (for more details of DX-COM, refer to the user guide ðŸ‘‰  [here](https://developer.deepx.ai/?files=MjUxNQ==))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca7205-5cef-453c-863a-794925e61820",
   "metadata": {},
   "source": [
    "The source structure of DX-COM is organized as follows:\n",
    "```bash\n",
    "dx_com\n",
    " â”œâ”€â”€ calibration_dataset   # Dataset used to optimize model accuracy <br>\n",
    " â”œâ”€â”€ dx_com\n",
    " â”‚ â”œâ”€â”€ cv2/                # Third party shared libraries (e.g., OpenCV) <br>\n",
    " â”‚ â”œâ”€â”€ google/             # Third party shared libraries (e.g., protobuf) <br>\n",
    " â”‚ â”œâ”€â”€ numpy/              # Third party shared libraries (e.g., NumPy) <br>\n",
    " â”‚ â”œâ”€â”€ ...                 # Other dependencies <br>\n",
    " â”‚ â””â”€â”€ dx_com              # Core compiler implementation <br>\n",
    " â”œâ”€â”€ sample\n",
    " â”‚ â”œâ”€â”€ MobilenetV1.json    # Sample configuration file <br>\n",
    " â”‚ â””â”€â”€ MobilenetV1.onnx    # Sample ONNX model\n",
    " â””â”€â”€ Makefile              # Build script for compiling the sample model <br>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cb05a-ee64-4392-9a09-3b0577e34a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pwd\n",
    "# Move to dx_com directory\n",
    "%cd dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93624034-2283-4052-b98d-2abf9d0fb797",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 1\n",
    "!tree sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb81091-638a-4c88-b9ac-04cd85bbef9d",
   "metadata": {},
   "source": [
    "!cat "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78f67a-62bb-4b05-8260-3a0820973a08",
   "metadata": {},
   "source": [
    "How to create my custom YOLOv7 model:\n",
    "- [Link](https://colab.research.google.com/drive/1dAdjJuhXqFM_Qcd0QqAn7_AGx7abA5aX?usp=sharing)\n",
    "- [DXNN](https://drive.google.com/file/d/1e9PEJeS2ZM16Y-5U2HrG9o5jzdvEaIh0/view?usp=sharing)\n",
    "- [ONNX](https://drive.google.com/file/d/1ZlRppHtz26X1ID8BUHuFMH6EVfPzDOsh/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c51947-4547-42de-82c7-f6996195c61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/donggyun/git/dx-tutorials/dx-all-suite/workspace/release/dx_com/dx_com_M1_v2.0.0\n"
     ]
    }
   ],
   "source": [
    "!cp ~/Downloads/yolov7-forklift-person.dxnn ./\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40a20b16-34f5-45c9-975b-d732bebfa025",
   "metadata": {},
   "source": [
    "%cd home/donggyun/git/dx-tutorials/dx-all-suite/dx-runtime/dx_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1290c6b6-7072-4086-b99c-df7da7ed5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/Downloads/yolov7-forklift-person.dxnn ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27bd22-50ed-4882-a280-b4cc9d056bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
