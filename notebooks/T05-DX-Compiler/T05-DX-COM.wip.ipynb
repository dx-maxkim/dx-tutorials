{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71513b35-9e60-4c49-b3ac-45c66d7f3933",
   "metadata": {},
   "source": [
    "# DEEPX Tutorial 05 - DX Compile workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38306cf-27f3-454b-9794-eb4ce4ee76a3",
   "metadata": {},
   "source": [
    "In Tutorial 5, you will practice compiling classification, object detection, and segmentation models using the DX-Compiler. We will also review the guide for troubleshooting problems during compilation.\n",
    "\n",
    "For more details, refer to the DX-COM user guide üëâ [Download](https://developer.deepx.ai/?files=MjUxNQ==)\n",
    "\n",
    ">This tutorial is based on dx-all-suite v2.0.0, released in September 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3d09f-4ccd-4589-8c7a-aa58cfd510fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Compiling Image Classification Model (MobileNetV2)\n",
    "1. Export PyTorch ‚Üí ONNX\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cbee8-6cbc-4a78-8700-1cdc65076c4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Export PyTorch ‚Üí ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b5de9-ab8a-466e-bfba-823f0fe96817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch & onnx\n",
    "!pip install --quiet torch torchvision onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87986a8c-af2b-43f5-8860-c63f0bc14ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639e9cb-7a79-422a-93e4-700ad8fcf472",
   "metadata": {},
   "source": [
    "Export Pytorch based MobileNetV2 model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e609d-ba12-45e5-8f40-f0d443165d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "# Load MobileNetV2 model\n",
    "model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Batch size must be 1\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "onnx_path = \"MobilenetV2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                      # PyTorch model object to export\n",
    "    dummy_input,                # Dummy input used for tracing (tuple is possible)\n",
    "    onnx_path,                  # Output ONNX file path\n",
    "    export_params=True,         # If True, saves model parameter (weight) into the ONNX file\n",
    "    input_names=[\"input_test\"], # Name of the ONNX model input tensor\n",
    "    output_names=[\"output\"],    # Name of the ONNX model output tensor\n",
    "    opset_version=13            # ONNX opset version (recommended: 11 ~ 21)\n",
    ")\n",
    "print(\"‚úÖ Save ONNX:\", onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435e445-e87e-4706-ad8f-09d040477cb0",
   "metadata": {},
   "source": [
    "### 2. JSON configuration for Input/Pre-processing/Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e037ae0-6ebd-4429-a5a6-70d56c9f818c",
   "metadata": {},
   "source": [
    "Generate a configuration file for Input/Pre-processing/Calibration of MobilenetV2.\n",
    "\n",
    "This JSON configuration file incudes:\n",
    " - Input specifications\n",
    " - Calibration methods\n",
    " - Data preprocessing settings\n",
    " - Optional parameters for advanced compilation schemes\n",
    "\n",
    "Model **Input Restrictions**:\n",
    " - The batch size must be fixed to 1\n",
    " - Only a single input is supported (Multi-input will be supported in 2026)\n",
    " - Input name must exactly match ONNX model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e10ab-7804-4c31-b4d0-e39dd16749d9",
   "metadata": {},
   "source": [
    "#### 2.1. Incorrect input name case\n",
    "\n",
    "This MobilenetV2 model's input name is `input_test` as shown below:\n",
    "\n",
    "![](assets/mobilenetv2-input-name.png)\n",
    "\n",
    "Input name (`input_test` in this example) must exactly match the input name of JSON configuration.\n",
    "\n",
    "However, you can see there is wrong input name `incorrect_input_name` in the following JSON configuration for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe57234-a230-4925-be56-b3a410f26ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"incorrect_input_name\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47876a-1379-4bce-be6c-6889a0cb1397",
   "metadata": {},
   "source": [
    "Let's run DX-COM with this wrong JSON configuration. You will meet ERROR:\n",
    "> ConfigInputError: The input name in config incorrect_input_name is not same as model input input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a8e81-84d6-4e3d-93ba-e7026b4282cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01180d0a-2bfc-4ef8-8ed0-be93cbc8f755",
   "metadata": {},
   "source": [
    "#### 2.2. Incorrect input shape\n",
    "In the following JSON file, the input name has the correct one - `input_test`.\n",
    "\n",
    "However, even if your input shape is 1x3x224x224 (BxCxHxW), calibration image shape has 224x224x3 (HxWxC). You must change the shape of calibraiton image to match with 1x3x224x224 (BxCxHxW) by using `transpose` and `expandDim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655c076-5ac0-4049-bac6-b49ec98dce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"input_test\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc83888-67b8-4a2a-a8f2-b52787d2766e",
   "metadata": {},
   "source": [
    "Let's run DX-COM with this wrong JSON configuration. You will meet ERROR:\n",
    "\n",
    "> ConfigInputError: Config shape [1, 3, 224, 224] does not match preprocessed data shape [1, 224, 224, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ac0f7-0b8e-4dce-b9be-d31586657888",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57ef8e-0d99-425a-85c7-390480494f4c",
   "metadata": {},
   "source": [
    "#### 2.3. Add `transpose` & `expanDim` to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bbd86f-e2c9-4aef-8c4d-96ed3974caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"input_test\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cd446-8d2e-4c74-98ba-1e481eb57cfc",
   "metadata": {},
   "source": [
    "### 3. Compile with DX-Compiler and verify .dxnn\n",
    "Compile with dx_com to generate .dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2270e5a-6b04-457b-a7b1-33a3948515d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2907f72-5dc2-435b-9dc5-7b319a8bd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m MobilenetV2.dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30a3ea-3335-4809-9b65-e0ea0479db29",
   "metadata": {},
   "source": [
    "## Compiling Object Detection Model (YOLOv9s)\n",
    "1. Export PyTorch ‚Üí ONNX\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133bb6f-807e-43ba-8fa9-1c55b5691280",
   "metadata": {},
   "source": [
    "### 1. Export PyTorch ‚Üí ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b15e4a-d1dd-4968-a046-e5217b12972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3dbaa-e9b2-48ea-aacf-b08f7f26124b",
   "metadata": {},
   "source": [
    "#### 1.1. Download `yolov9-s.pt` pytorch model to your local system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cfb8d-1f8e-4e70-8d2a-28dfaff0c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd yolov9 && pip install -r requirements.txt\n",
    "!pip install torch==2.5.1 torchvision==0.20.1\n",
    "!pip install onnx onnxsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae6a3d-d15c-4861-9794-dedd0ad4aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-s.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218e930-06a4-4be6-bf02-80f64c150f34",
   "metadata": {},
   "source": [
    "#### 1.2. Export the downloaded torch model to ONNX:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c0d0b-c1d9-4128-8c0a-afc36fbcf020",
   "metadata": {},
   "source": [
    "Download yolov9 git repo to use `export.py` for yolov9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581e7e5-8d01-4bea-b11b-a59d2de35968",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov9.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4799a-bd6a-4551-bded-724390318036",
   "metadata": {},
   "source": [
    "Export Pytorch based `yolov9-s` model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9d87a-9ce4-40c5-8722-758e356e817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd yolov9 && python3 export.py --weights ../yolov9-s.pt \\\n",
    "                                --img-size 640 640 \\\n",
    "                                --opset 12 \\\n",
    "                                --simplify \\\n",
    "                                --batch-size 1 \\\n",
    "                                --include onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee5d9f-7328-49a2-bfc5-77eef6ac9b44",
   "metadata": {},
   "source": [
    "Check if `yolov9-s.onnx` file is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d853b-16f6-4d36-8419-582d7ec034b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls | grep yolov9-s.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4186075-a2ec-4147-80a7-bac3ebe3608d",
   "metadata": {},
   "source": [
    "### 2. JSON configuration for Input/Pre-processing/Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c6bc5-01a4-443b-9a0c-4e0ea048acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile yolov9-s.json\n",
    "{\n",
    "  \"inputs\": {\"images\": [1,3,640,640]},\n",
    "  \"calibration_num\": 100,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"div\": {\"x\": 255}},\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bd2bb-83b2-4646-80ec-f0aa528bbe61",
   "metadata": {},
   "source": [
    "### 3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56efd1b-4e30-436a-87ff-96f6e2aef06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m yolov9-s.onnx -c yolov9-s.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdea9f5-9c9a-42f5-a397-3b1d6a2be817",
   "metadata": {},
   "source": [
    "Check if **yolov9-s.dxnn** file is generated as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9074fe4-a5b0-44e9-91f7-af519106e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls | grep yolov9-s.dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f2f50a-5fe9-49af-9cb9-bac6fe82eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m yolov9-s.dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e1b99-96ca-412e-98bc-304e57c6c6bf",
   "metadata": {},
   "source": [
    "## Compiling ViT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca25b64-3bfd-4c60-86a7-84a3c240618a",
   "metadata": {},
   "source": [
    "https://github.com/mlfoundations/open_clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56856b2f-2886-4925-8e04-b82532d09d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/max/Works/dx-tutorials/dx-all-suite/workspace/release/dx_com/dx_com_M1_v2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a0c4d-8094-4d72-ab5c-e825ade41510",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/clip.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeeb117-4379-485a-bcdd-f1d92d832b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0aa3ba3-709f-41d7-b4a1-6798ea331695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
      "Î™®Îç∏Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î°úÎìúÎêòÏóàÏäµÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "# ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Ïù¥Î¶Ñ ÌôïÏù∏\n",
    "available_models = clip.available_models()\n",
    "print(available_models)\n",
    "# Ï∂úÎ†• Ïòà: ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', \n",
    "#          'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "\n",
    "# Ïû•Ïπò ÏÑ§Ï†ï (GPU ÏÇ¨Ïö© Í∞ÄÎä• Ïãú \"cuda\", ÏïÑÎãê Í≤ΩÏö∞ \"cpu\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ÌäπÏ†ï Î™®Îç∏ Î°úÎìú (Ïòà: \"ViT-B/32\")\n",
    "# Ïù¥ ÏãúÏ†êÏóê Î™®Îç∏Ïù¥ Î°úÏª¨Ïóê ÏóÜÏúºÎ©¥ ÏûêÎèôÏúºÎ°ú Îã§Ïö¥Î°úÎìúÎê©ÎãàÎã§.\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(\"Î™®Îç∏Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î°úÎìúÎêòÏóàÏäµÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c43e8d53-9aad-4723-a0ac-3df0d6663dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded successfully.\n",
      "Model dtype: torch.float32\n",
      "Image resolution: 224\n",
      "Text context length: 77\n",
      "\n",
      "Exporting Image Encoder to clip_image_encoder.onnx...\n",
      "Image Encoder export complete.\n",
      "\n",
      "Exporting Text Encoder to clip_text_encoder.onnx...\n",
      "Text Encoder export complete.\n",
      "\n",
      "Verifying ONNX models...\n",
      "‚úÖ Image Encoder ONNX verification SUCCESSFUL!\n",
      "‚úÖ Text Encoder ONNX verification SUCCESSFUL!\n",
      "\n",
      "All tasks finished.\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "import os\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "model.eval() # Î™®Îç∏ÏùÑ ÌèâÍ∞Ä Î™®ÎìúÎ°ú ÏÑ§Ï†ï (ÌïÑÏàò)\n",
    "print(\"CLIP model loaded successfully.\")\n",
    "\n",
    "# --- 2. ONNX ÎÇ¥Î≥¥ÎÇ¥Í∏∞ ÏÑ§Ï†ï ---\n",
    "ONNX_OPSET_VERSION = 20\n",
    "IMAGE_ONNX_PATH = \"clip_image_encoder.onnx\"\n",
    "TEXT_ONNX_PATH = \"clip_text_encoder.onnx\"\n",
    "\n",
    "# Î™®Îç∏Ïùò ÏûÖÎ†• Ìï¥ÏÉÅÎèÑ Î∞è Ïª®ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "image_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "model_dtype = model.dtype # Î™®Îç∏Ïùò Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ (cudaÏùº Í≤ΩÏö∞ float16, cpuÏùº Í≤ΩÏö∞ float32)\n",
    "\n",
    "print(f\"Model dtype: {model_dtype}\")\n",
    "print(f\"Image resolution: {image_resolution}\")\n",
    "print(f\"Text context length: {context_length}\")\n",
    "\n",
    "# --- 3. Image Encoder (Visual) ONNX Î≥ÄÌôò ---\n",
    "print(f\"\\nExporting Image Encoder to {IMAGE_ONNX_PATH}...\")\n",
    "\n",
    "# 1. ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ ÏûÖÎ†• ÏÉùÏÑ±\n",
    "#    Î∞∞Ïπò ÌÅ¨Í∏∞ 1, 3Í∞ú Ï±ÑÎÑê, Î™®Îç∏ Ìï¥ÏÉÅÎèÑ\n",
    "dummy_image_input = torch.randn(\n",
    "    1, 3, image_resolution, image_resolution, \n",
    "    device=device, dtype=model_dtype\n",
    ")\n",
    "\n",
    "# 2. ONNX Î≥ÄÌôò Ïã§Ìñâ\n",
    "torch.onnx.export(\n",
    "    model.visual,                 # ÎÇ¥Î≥¥ÎÇº Î™®Îç∏ (Image Encoder)\n",
    "    dummy_image_input,            # ÎçîÎØ∏ ÏûÖÎ†•\n",
    "    IMAGE_ONNX_PATH,              # Ï†ÄÏû• Í≤ΩÎ°ú\n",
    "    opset_version=ONNX_OPSET_VERSION,\n",
    "    input_names=['image'],        # ÏûÖÎ†• Î†àÏù¥Ïñ¥ Ïù¥Î¶Ñ\n",
    "    output_names=['image_features'] # Ï∂úÎ†• Î†àÏù¥Ïñ¥ Ïù¥Î¶Ñ\n",
    ")\n",
    "print(\"Image Encoder export complete.\")\n",
    "\n",
    "\n",
    "# --- 4. Text Encoder ONNX Î≥ÄÌôò ---\n",
    "print(f\"\\nExporting Text Encoder to {TEXT_ONNX_PATH}...\")\n",
    "\n",
    "# 1. Text Encoder ÎûòÌçº ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "# Ïù¥Ïú†: model.encode_textÎäî Îã®Ïàú nn.ModuleÏù¥ ÏïÑÎãàÎùº, \n",
    "#      Ïó¨Îü¨ Î™®Îìà(embedding, transformer, projection)ÏùÑ ÏàúÏ∞®Ï†ÅÏúºÎ°ú Ìò∏Ï∂úÌïòÎäî 'Î©îÏÑúÎìú'ÏûÖÎãàÎã§.\n",
    "#      ONNXÎ°ú ÎÇ¥Î≥¥ÎÇ¥Î†§Î©¥ Ïù¥ Î°úÏßÅÏùÑ forward() Î©îÏÑúÎìúÎ°ú Í∞êÏã∏Ïïº Ìï©ÎãàÎã§.\n",
    "class TextEncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.token_embedding = clip_model.token_embedding\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, text):\n",
    "        # clip.model.CLIP.encode_textÏùò Î°úÏßÅÏùÑ Í∑∏ÎåÄÎ°ú Íµ¨ÌòÑ\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "        \n",
    "        # positional_embeddingÏùÄ [n_ctx, d_model]\n",
    "        x = x + self.positional_embedding[:x.size(1)].type(self.dtype)\n",
    "        \n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # EOT ÌÜ†ÌÅ∞Ïùò ÌîºÏ≤òÎ•º Í∞ÄÏ†∏ÏòµÎãàÎã§.\n",
    "        # clip.tokenize()Îäî EOT ÌÜ†ÌÅ∞(49407)Ïù¥ Í∞ÄÏû• ÌÅ¨Í≥† Ìå®Îî©ÏùÄ 0ÏúºÎ°ú Ï±ÑÏö∞ÎØÄÎ°ú argmaxÎ°ú Ï∞æÏäµÎãàÎã§.\n",
    "        batch_indices = torch.arange(x.size(0), device=text.device)\n",
    "        eot_indices = text.argmax(dim=-1)\n",
    "        x = x[batch_indices, eot_indices] \n",
    "        \n",
    "        # ÏµúÏ¢Ö ÌîÑÎ°úÏ†ùÏÖò\n",
    "        x = x @ self.text_projection\n",
    "        return x\n",
    "\n",
    "# 2. ÎûòÌçº Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± Î∞è ÎçîÎØ∏ ÌÖçÏä§Ìä∏ ÏûÖÎ†• ÏÉùÏÑ±\n",
    "text_encoder = TextEncoderWrapper(model).to(device).eval()\n",
    "dummy_text_input = clip.tokenize([\"a test sentence for onnx export\"]).to(device) # (1, 77)\n",
    "\n",
    "# 3. ONNX Î≥ÄÌôò Ïã§Ìñâ\n",
    "torch.onnx.export(\n",
    "    text_encoder,                 # ÎÇ¥Î≥¥ÎÇº Î™®Îç∏ (Text Encoder ÎûòÌçº)\n",
    "    dummy_text_input,             # ÎçîÎØ∏ ÏûÖÎ†•\n",
    "    TEXT_ONNX_PATH,               # Ï†ÄÏû• Í≤ΩÎ°ú\n",
    "    opset_version=ONNX_OPSET_VERSION,\n",
    "    input_names=['text'],         # ÏûÖÎ†• Î†àÏù¥Ïñ¥ Ïù¥Î¶Ñ (ÌÜ†ÌÅ∞ ID)\n",
    "    output_names=['text_features'] # Ï∂úÎ†• Î†àÏù¥Ïñ¥ Ïù¥Î¶Ñ\n",
    ")\n",
    "print(\"Text Encoder export complete.\")\n",
    "\n",
    "\n",
    "# --- 5. (ÏÑ†ÌÉù) ONNX Î™®Îç∏ Í≤ÄÏ¶ù ---\n",
    "print(\"\\nVerifying ONNX models...\")\n",
    "\n",
    "# CPUÏóêÏÑú numpyÎ°ú ÎπÑÍµê (GPU/CPU, fp16/fp32 ÌÉÄÏûÖ Î∂àÏùºÏπò Î¨∏Ï†ú Î∞©ÏßÄ)\n",
    "device_for_verify = \"cpu\"\n",
    "model.to(device_for_verify)\n",
    "text_encoder.to(device_for_verify)\n",
    "\n",
    "# PyTorch ÏõêÎ≥∏ Î™®Îç∏ Ï∂úÎ†•\n",
    "with torch.no_grad():\n",
    "    # Ïã§Ï†ú ÌÖåÏä§Ìä∏Ïö© ÏûÖÎ†•\n",
    "    test_image_input_dummy = torch.randn(1, 3, image_resolution, image_resolution, device=device_for_verify)\n",
    "    test_text_input = clip.tokenize([\"a photo of a cat\"]).to(device_for_verify) # (1, 77)\n",
    "\n",
    "    # Ïù¥ÎØ∏ÏßÄ Î™®Îç∏ÏùÄ fp32Î°ú Î≥ÄÌôò ÌõÑ Ïã§Ìñâ (ONNX Îü∞ÌÉÄÏûÑÏù¥ Î≥¥ÌÜµ fp32Î°ú Ïã§ÌñâÎê®)\n",
    "    torch_image_out = model.visual(test_image_input_dummy.float()).cpu().numpy()\n",
    "    \n",
    "    # ÌÖçÏä§Ìä∏ Î™®Îç∏ Ïã§Ìñâ\n",
    "    torch_text_out = text_encoder(test_text_input).cpu().numpy()\n",
    "\n",
    "\n",
    "# ONNX Îü∞ÌÉÄÏûÑ ÏÑ∏ÏÖò ÏÉùÏÑ± Î∞è Ï∂úÎ†•\n",
    "ort_image_session = onnxruntime.InferenceSession(IMAGE_ONNX_PATH, providers=['CPUExecutionProvider'])\n",
    "ort_text_session = onnxruntime.InferenceSession(TEXT_ONNX_PATH, providers=['CPUExecutionProvider'])\n",
    "\n",
    "# ONNX Î™®Îç∏ Ïã§Ìñâ\n",
    "# onnx Îü∞ÌÉÄÏûÑÏùÄ numpy Î∞∞Ïó¥ÏùÑ ÏûÖÎ†•ÏúºÎ°ú Î∞õÏäµÎãàÎã§.\n",
    "ort_image_input = {'image': test_image_input_dummy.float().cpu().numpy()}\n",
    "ort_text_input = {'text': test_text_input.cpu().numpy()}\n",
    "\n",
    "ort_image_out = ort_image_session.run(['image_features'], ort_image_input)[0]\n",
    "ort_text_out = ort_text_session.run(['text_features'], ort_text_input)[0]\n",
    "\n",
    "# Í≤∞Í≥º ÎπÑÍµê\n",
    "try:\n",
    "    # atol (Ï†àÎåÄ Ïò§Ï∞®) Í∞íÏùÑ 1e-4 ÎòêÎäî 1e-5 Ï†ïÎèÑÎ°ú ÏÑ§Ï†ï\n",
    "    np.testing.assert_allclose(torch_image_out, ort_image_out, rtol=1e-03, atol=1e-04)\n",
    "    print(\"‚úÖ Image Encoder ONNX verification SUCCESSFUL!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Image Encoder ONNX verification FAILED: {e}\")\n",
    "\n",
    "try:\n",
    "    np.testing.assert_allclose(torch_text_out, ort_text_out, rtol=1e-03, atol=1e-04)\n",
    "    print(\"‚úÖ Text Encoder ONNX verification SUCCESSFUL!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Text Encoder ONNX verification FAILED: {e}\")\n",
    "\n",
    "print(\"\\nAll tasks finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6206e0e5-17d3-49ed-8086-ffbe2852437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Step 6: End-to-End ONNX Inference ---\n",
      "Loaded ONNX sessions and logit_scale (100.0000)\n",
      "Image preprocessed. Shape: (1, 3, 224, 224), Dtype: float32\n",
      "Text tokenized. Shape: (4, 77), Dtype: int32\n",
      "Image features extracted. Shape: (1, 512)\n",
      "Text batch size is 4. Running inference in a loop...\n",
      "Text features extracted (from loop). Shape: (4, 512)\n",
      "\n",
      "Cosine Similarity Scores (raw):\n",
      "[[0.19304886 0.24570042 0.17482775 0.18901584]]\n",
      "\n",
      "--- Final Probabilities ---\n",
      "a photo of a dog    : 0.51%\n",
      "a photo of a cat    : 99.06%\n",
      "a photo of a bird   : 0.08%\n",
      "a car               : 0.34%\n",
      "\n",
      "‚úÖ Best Match: a photo of a cat\n",
      "\n",
      "--- ONNX End-to-End Inference Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- 6. ONNX Î™®Îç∏ÏùÑ Ïù¥Ïö©Ìïú Ïã§Ï†ú Ï∂îÎ°† (End-to-End) ---\n",
    "\n",
    "# onnxruntime, numpy, PIL, clip, urllib Îì± ÌïÑÏöîÌïú Î™®Îìà Ïû¨ÌôïÏù∏\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import clip\n",
    "import torch\n",
    "import urllib.request\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"\\n--- Starting Step 6: End-to-End ONNX Inference ---\")\n",
    "\n",
    "# --- 6-1. ÏÑ§Ï†ï Î∞è Î™®Îç∏ Î°úÎìú ---\n",
    "\n",
    "# ÏõêÎ≥∏ PyTorch Î™®Îç∏ÏóêÏÑú Ï†ÑÏ≤òÎ¶¨Í∏∞(preprocess)ÏôÄ logit_scaleÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§.\n",
    "# (ÏÇ¨Ïö©ÏûêÎãòÏùò Í∏∞Ï°¥ ÏΩîÎìúÏóêÏÑú 'model'Í≥º 'preprocess' Î≥ÄÏàòÍ∞Ä Ïù¥ÎØ∏ Î°úÎìúÎêòÏóàÎã§Í≥† Í∞ÄÏ†ïÌï©ÎãàÎã§)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load(\"ViT-B/32\", device=device) # Ïù¥ Î∂ÄÎ∂ÑÏù¥ Ïù¥ÎØ∏ Ïã§ÌñâÎê®\n",
    "\n",
    "# ONNX ÌååÏùº Í≤ΩÎ°ú\n",
    "IMAGE_ONNX_PATH = \"clip_image_encoder.onnx\"\n",
    "TEXT_ONNX_PATH = \"clip_text_encoder.onnx\"\n",
    "\n",
    "# ONNX Îü∞ÌÉÄÏûÑ ÏÑ∏ÏÖò Î°úÎìú (CPUExecutionProvider ÏÇ¨Ïö©)\n",
    "# (Í≤ÄÏ¶ù Îã®Í≥ÑÏóêÏÑú Ïù¥ÎØ∏ ÏÉùÏÑ±ÌñàÏßÄÎßå, Ï∂îÎ°† ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÏúÑÌï¥ Î™ÖÏãúÏ†ÅÏúºÎ°ú Îã§Ïãú Î°úÎìú)\n",
    "providers = ['CPUExecutionProvider']\n",
    "image_session = onnxruntime.InferenceSession(IMAGE_ONNX_PATH, providers=providers)\n",
    "text_session = onnxruntime.InferenceSession(TEXT_ONNX_PATH, providers=providers)\n",
    "\n",
    "# CLIPÏùò logit_scale Í∞íÏùÑ ÏõêÎ≥∏ Î™®Îç∏ÏóêÏÑú Í∞ÄÏ†∏ÏòµÎãàÎã§.\n",
    "# Ïù¥ Í∞íÏùÄ Ïú†ÏÇ¨ÎèÑ(cosine similarity)Î•º ÏµúÏ¢Ö ÌôïÎ•†Î°ú Î≥ÄÌôòÌï† Îïå ÌïÑÏöîÌï©ÎãàÎã§.\n",
    "# .item()ÏùÑ ÏÇ¨Ïö©Ìï¥ Python Ïà´ÏûêÎ°ú Î≥ÄÌôòÌï©ÎãàÎã§.\n",
    "# Ïù¥ÎØ∏ device_for_verify = \"cpu\"Î°ú Î™®Îç∏ÏùÑ ÏòÆÍ≤ºÏúºÎØÄÎ°ú .cpu()Îäî ÌïÑÏöî ÏóÜÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\n",
    "try:\n",
    "    logit_scale = model.logit_scale.exp().item()\n",
    "except AttributeError:\n",
    "    # ÌòπÏãú modelÏù¥ CPUÏóê ÏóÜÎäî Í≤ΩÏö∞ ÎåÄÎπÑ\n",
    "    logit_scale = model.to(\"cpu\").logit_scale.exp().item()\n",
    "\n",
    "print(f\"Loaded ONNX sessions and logit_scale ({logit_scale:.4f})\")\n",
    "\n",
    "\n",
    "# --- 6-2. Ìó¨Ìçº Ìï®Ïàò Ï†ïÏùò (Numpy Í∏∞Î∞ò) ---\n",
    "\n",
    "def normalize(features):\n",
    "    \"\"\"Numpy Î∞∞Ïó¥ÏùÑ L2 Ï†ïÍ∑úÌôîÌï©ÎãàÎã§.\"\"\"\n",
    "    norm = np.linalg.norm(features, axis=1, keepdims=True)\n",
    "    return features / norm\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Numpy Î∞∞Ïó¥Ïóê softmaxÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "# --- 6-3. ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ ---\n",
    "\n",
    "# 1. ÏÉòÌîå Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\" # Í≥†ÏñëÏù¥ ÏÇ¨ÏßÑ\n",
    "image_path = \"cat_sample.jpg\"\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Downloading sample image to {image_path}...\")\n",
    "    urllib.request.urlretrieve(image_url, image_path)\n",
    "\n",
    "# PIL Ïù¥ÎØ∏ÏßÄ Ïó¥Í∏∞\n",
    "pil_image = Image.open(image_path)\n",
    "\n",
    "# CLIPÏùò 'preprocess' Ìï®ÏàòÎ°ú Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨\n",
    "# preprocessÎäî (224, 224) ÌÅ¨Í∏∞Ïùò PyTorch ÌÖêÏÑúÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n",
    "image_tensor = preprocess(pil_image).unsqueeze(0) # (1, 3, 224, 224)\n",
    "\n",
    "# ONNX Îü∞ÌÉÄÏûÑÏùÄ numpy Î∞∞Ïó¥ÏùÑ ÏûÖÎ†•ÏúºÎ°ú Î∞õÏäµÎãàÎã§.\n",
    "# Í≤ÄÏ¶ù Îã®Í≥Ñ(5)ÏóêÏÑú .float()ÏùÑ ÏÇ¨Ïö©ÌñàÏúºÎØÄÎ°ú Ïó¨Í∏∞ÏÑúÎèÑ ÎèôÏùºÌïòÍ≤å fp32Î°ú Î≥ÄÌôòÌï©ÎãàÎã§.\n",
    "# (ÏõêÎ≥∏ Î™®Îç∏Ïù¥ fp16Ïù¥ÏóàÎçîÎùºÎèÑ ONNXÍ∞Ä fp32 ÏûÖÎ†•ÏùÑ Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏùå)\n",
    "image_input_np = image_tensor.float().cpu().numpy()\n",
    "print(f\"Image preprocessed. Shape: {image_input_np.shape}, Dtype: {image_input_np.dtype}\")\n",
    "\n",
    "\n",
    "# 2. ÌÖçÏä§Ìä∏ Î†àÏù¥Î∏î Ï§ÄÎπÑ Î∞è ÌÜ†ÌÅ∞Ìôî\n",
    "text_labels = [\"a photo of a dog\", \"a photo of a cat\", \"a photo of a bird\", \"a car\"]\n",
    "text_tokens = clip.tokenize(text_labels) # (4, 77)\n",
    "\n",
    "# ONNX Îü∞ÌÉÄÏûÑÏö© numpy Î∞∞Ïó¥Î°ú Î≥ÄÌôò (int64)\n",
    "text_input_np = text_tokens.cpu().numpy()\n",
    "print(f\"Text tokenized. Shape: {text_input_np.shape}, Dtype: {text_input_np.dtype}\")\n",
    "\n",
    "\n",
    "# --- 6-4. ONNX Ï∂îÎ°† Ïã§Ìñâ ---\n",
    "\n",
    "# 1. Ïù¥ÎØ∏ÏßÄ ÌîºÏ≤ò Ï∂îÏ∂ú\n",
    "image_input_dict = {'image': image_input_np}\n",
    "image_features_np = image_session.run(['image_features'], image_input_dict)[0]\n",
    "print(f\"Image features extracted. Shape: {image_features_np.shape}\")\n",
    "\n",
    "# 2. ÌÖçÏä§Ìä∏ ÌîºÏ≤ò Ï∂îÏ∂ú\n",
    "text_features_list = []\n",
    "print(f\"Text batch size is {text_input_np_int64.shape[0]}. Running inference in a loop...\")\n",
    "\n",
    "for i in range(text_input_np_int64.shape[0]):\n",
    "    # [i:i+1] Ïä¨ÎùºÏù¥Ïã±ÏùÑ ÏÇ¨Ïö©Ìï¥ (1, 77) ÌòïÌÉúÎ•º Ïú†ÏßÄÌïòÎ©∞ ÌïòÎÇòÏî© Ï∂îÏ∂ú\n",
    "    single_text_input = text_input_np[i:i+1]\n",
    "    # ÏïàÏ†ÑÏùÑ ÏúÑÌï¥ ÏûÖÎ†• ÌÉÄÏûÖÏùÑ np.int32Î°ú Î™ÖÏãúÏ†ÅÏúºÎ°ú Î≥ÄÌôòÌï©ÎãàÎã§.\n",
    "    single_text_input_int32 = single_text_input.astype(np.int32)\n",
    "    \n",
    "    text_input_dict = {'text': single_text_input_int32}\n",
    "    # ÌÖçÏä§Ìä∏ ÌîºÏ≤òÎ•º ÌïòÎÇòÏî© Ï∂îÎ°†\n",
    "    single_text_features = text_session.run(['text_features'], text_input_dict)[0]\n",
    "    \n",
    "    text_features_list.append(single_text_features)\n",
    "\n",
    "# Ï∂îÎ°†Îêú ÌîºÏ≤òÎì§ÏùÑ Îã§Ïãú ÌïòÎÇòÏùò numpy Î∞∞Ïó¥Î°ú Ìï©Ïπ©ÎãàÎã§.\n",
    "text_features_np = np.vstack(text_features_list)\n",
    "print(f\"Text features extracted (from loop). Shape: {text_features_np.shape}\")\n",
    "\n",
    "\n",
    "# --- 6-5. Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ Î∞è Í≤∞Í≥º ÌôïÏù∏ ---\n",
    "\n",
    "# 1. ÌîºÏ≤ò Ï†ïÍ∑úÌôî (L2 Normalization)\n",
    "image_features_norm = normalize(image_features_np)\n",
    "text_features_norm = normalize(text_features_np)\n",
    "\n",
    "# 2. ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ (ÌñâÎ†¨ Í≥±)\n",
    "# (1, 512) @ (512, 4) -> (1, 4)\n",
    "similarity_scores = image_features_norm @ text_features_norm.T\n",
    "print(f\"\\nCosine Similarity Scores (raw):\\n{similarity_scores}\")\n",
    "\n",
    "# 3. ÏµúÏ¢Ö ÌôïÎ•† Í≥ÑÏÇ∞ (logit_scale Ï†ÅÏö© Î∞è softmax)\n",
    "logits = similarity_scores * logit_scale\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(f\"\\n--- Final Probabilities ---\")\n",
    "for label, prob in zip(text_labels, probabilities[0]):\n",
    "    print(f\"{label:<20}: {prob * 100:.2f}%\")\n",
    "\n",
    "# Í≤∞Í≥º ÌôïÏù∏\n",
    "best_match_index = np.argmax(probabilities[0])\n",
    "print(f\"\\n‚úÖ Best Match: {text_labels[best_match_index]}\")\n",
    "\n",
    "print(\"\\n--- ONNX End-to-End Inference Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a535045-8f6e-4ece-863b-80410f01a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clip_image.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile clip_image.json\n",
    "{\n",
    "  \"inputs\":\n",
    "  {\n",
    "    \"image\": [1, 3, 224, 224]},\n",
    "    \"default_loader\": \n",
    "    {\n",
    "      \"dataset_path\": \"/home/max/Works/dx-tutorials/dx-all-suite/dx-compiler/dx_com/calibration_dataset\",\n",
    "      \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "      \"preprocessings\": [{\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "                         {\"centercrop\": {\"width\": 224, \"height\": 224}},\n",
    "                         {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "                         {\"div\": {\"x\": 255.0}},\n",
    "                         {\"normalize\":\n",
    "                           {\"mean\": [0.48145466, 0.4578275, 0.40821073],\n",
    "                            \"std\": [0.26862954, 0.26130258, 0.27577711]}},\n",
    "                         {\"transpose\": {\"axis\": [2, 0, 1]}},\n",
    "                         {\"expandDim\": {\"axis\": 0}}]},\n",
    "  \"calibration_num\": 100,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"train_batchsize\": 32,\n",
    "  \"num_samples\": 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a3de78-ff8b-414a-b854-1c0a2d83c573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibration_dataset\t clip_image.json\t dx_com   Makefile  sample\n",
      "clip_image_encoder.onnx  clip_text_encoder.onnx  LICENSE  output\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5cd1a7e-3136-4684-b0b4-f7d85028d1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Logger initialized in 'release' mode.\n",
      "[INFO] - Starting ONNX import for: clip_image_encoder.onnx                      \n",
      "[INFO] - Loading and validating model from: clip_image_encoder.onnx             \n",
      "[INFO] - Model loaded and validated successfully.                               \n",
      "[INFO] - Converting model opset to target 21...                                 \n",
      "[INFO] - Opset conversion successful.                                           \n",
      "[INFO] - Updating model IR version from 9 to 10.                                \n",
      "[INFO] - Successfully imported ONNX model!!!                                    \n",
      "[INFO] - --- Running Surgeon Stage ---                                          \n",
      "[INFO] - --- Running Canonicalizer Passes for NPU M1A ---                       \n",
      "[INFO] - Running PassManager with 48 rules.                                     \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - Canonicalizer finished. Applied 48 pass(es).                           \n",
      "[INFO] - --- Running Fuser Stage ---                                            \n",
      "[INFO] - Running PassManager with 15 rules.                                     \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - Fuser finished. Applied 15 pass(es).                                   \n",
      "[INFO] - --- Running Special Optimizer Stage ---                                \n",
      "[INFO] - Running PassManager with 3 rules.                                      \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - Special Optimizer finished. Applied 3 pass(es).                        \n",
      "[INFO] - --- Surgeon Stage Finished ---                                         \n",
      "[INFO] - --- Running Classifier ---                                             \n",
      "[INFO] - Starting node classification...                                        \n",
      "[INFO] - Node classification completed.                                         \n",
      "[INFO] - Classification result: 430 NPU nodes, 38 CPU nodes                     \n",
      "[INFO] - Starting partitioning of graph 'main_graph'                            \n",
      "[INFO] - Found 430 NPU nodes and 38 CPU nodes                                   \n",
      "[INFO] - Created 1 NPU graphs and 2 CPU graphs                                  \n",
      "[INFO] - Created 1 NPU graphs and 2 CPU graphs                                  \n",
      "[INFO] - Partitioning complete: 1 NPU graphs, 2 CPU graphs                      \n",
      "[INFO] - Generating graph info for partitioned graph 'merged_graph'             \n",
      "[INFO] - Successfully generated partitioned graph info in the target format.    \n",
      "[INFO] - --- Starting Quantization Stage ---                                    \n",
      "[INFO] - --- Running Fx Preapre Passes. ---                                     \n",
      "[INFO] - Running PassManager with 3 rules.                                      \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - Fx Prepare fisnished. Applied 3 pass(es).                              \n",
      "[INFO] - Starting IR to FX conversion for graph: npu_0                          \n",
      "[INFO] - IR to FX conversion successful.                                        \n",
      "[INFO] - [RUNNING] Pattern: CustomModuleInjectorRule                            \n",
      "[INFO] - [RUNNING] Pattern: Int8InputRule                                       \n",
      "[INFO] - [RUNNING] Pattern: FP32InInt8OutRule                                   \n",
      "[INFO] - [RUNNING] Pattern: ArgMaxRule                                          \n",
      "[INFO] - [RUNNING] Pattern: BothSideMainOpConvergeRule                          \n",
      "[INFO] - [RUNNING] Pattern: SkipSfuQuantRule                                    \n",
      "[INFO] - [RUNNING] Pattern: DivergeQuantRule                                    \n",
      "[INFO] - Phase 3: Running calibration on FX + ONNX hybrid model...              \n",
      "[INFO] - Starting calibration with 100 batches                                  \n",
      "Compiling Model :  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 0.4/1.0 [00:02<00:08, 14.72s/model ]\n",
      "Calibration:   0%|                                                                              | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Calibration:   3%|‚ñà‚ñà                                                                    | 3/100 [00:00<00:04, 21.69it/s]\u001b[A\n",
      "Calibration:   6%|‚ñà‚ñà‚ñà‚ñà‚ñè                                                                 | 6/100 [00:00<00:04, 21.38it/s]\u001b[A\n",
      "Calibration:   9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                               | 9/100 [00:00<00:04, 22.48it/s]\u001b[A\n",
      "Calibration:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                            | 12/100 [00:00<00:03, 22.79it/s]\u001b[A\n",
      "Calibration:  15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                          | 15/100 [00:00<00:03, 21.84it/s]\u001b[A\n",
      "Calibration:  18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                        | 18/100 [00:00<00:03, 21.96it/s]\u001b[A\n",
      "Calibration:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                      | 21/100 [00:00<00:03, 21.69it/s]\u001b[A\n",
      "Calibration:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                    | 24/100 [00:01<00:03, 20.97it/s]\u001b[A\n",
      "Calibration:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                  | 27/100 [00:01<00:03, 21.33it/s]\u001b[A\n",
      "Calibration:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                | 30/100 [00:01<00:03, 21.36it/s]\u001b[A\n",
      "Calibration:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                              | 33/100 [00:01<00:03, 19.50it/s]\u001b[A\n",
      "Calibration:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                            | 36/100 [00:01<00:03, 19.65it/s]\u001b[A\n",
      "Calibration:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                          | 38/100 [00:01<00:03, 17.56it/s]\u001b[A\n",
      "Calibration:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                        | 41/100 [00:02<00:03, 19.09it/s]\u001b[A\n",
      "Calibration:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                      | 44/100 [00:02<00:02, 19.63it/s]\u001b[A\n",
      "Calibration:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                     | 46/100 [00:02<00:02, 19.69it/s]\u001b[A\n",
      "Calibration:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 49/100 [00:02<00:02, 21.15it/s]\u001b[A\n",
      "Calibration:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 52/100 [00:02<00:02, 18.15it/s]\u001b[A\n",
      "Calibration:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 55/100 [00:02<00:02, 19.39it/s]\u001b[A\n",
      "Calibration:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 58/100 [00:02<00:02, 18.92it/s]\u001b[A\n",
      "Calibration:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 61/100 [00:03<00:01, 19.81it/s]\u001b[A\n",
      "Calibration:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 64/100 [00:03<00:01, 21.09it/s]\u001b[A\n",
      "Calibration:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 67/100 [00:03<00:01, 21.31it/s]\u001b[A\n",
      "Calibration:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 70/100 [00:03<00:01, 21.76it/s]\u001b[A\n",
      "Calibration:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 73/100 [00:03<00:01, 22.52it/s]\u001b[A\n",
      "Calibration:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 76/100 [00:03<00:01, 22.45it/s]\u001b[A\n",
      "Calibration:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 79/100 [00:03<00:00, 21.90it/s]\u001b[A\n",
      "Calibration:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 82/100 [00:03<00:00, 22.05it/s]\u001b[A\n",
      "Calibration:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 85/100 [00:04<00:00, 22.05it/s]\u001b[A\n",
      "Calibration:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 88/100 [00:04<00:00, 22.50it/s]\u001b[A\n",
      "Calibration:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 91/100 [00:04<00:00, 22.30it/s]\u001b[A\n",
      "Calibration:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 94/100 [00:04<00:00, 20.36it/s]\u001b[A\n",
      "Calibration:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 97/100 [00:04<00:00, 20.91it/s]\u001b[A\n",
      "Calibration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.80it/s]\u001b[A\n",
      "[INFO] - Calibration complete.                                                  \n",
      "[INFO] - Phase 4: Extracting quantization parameters...                         \n",
      "[INFO] - Extracting qparams from observers...                                   \n",
      "[INFO] - Processing subgraph: npu_0                                             \n",
      "[INFO] - Qparam extraction complete.                                            \n",
      "[INFO] - Phase 5 & 6: Updating weights and running quantization pipeline for all NPU graphs...\n",
      "[INFO] - --- Running Quantization Pipeline ---                                  \n",
      "[INFO] - Running PassManager with 1 rules.                                      \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - --- Pass InsertQuantNodesPass completed ---                            \n",
      "[INFO] - --- Pass QuantizationTypeInferencePass completed ---                   \n",
      "[INFO] - --- Pass InsertDeQuantNodesPass completed ---                          \n",
      "[INFO] - --- Starting Graph Transformation Passes ---                           \n",
      "[INFO] - Running PassManager with 1 rules.                                      \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - --- Pass 'UpdateQuantizedPadValuePass' completed. ---                  \n",
      "[INFO] - Running PassManager with 1 rules.                                      \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - --- Pass 'MoveConcatQuantPass' completed. ---                          \n",
      "[INFO] - Running PassManager with 1 rules.                                      \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - --- Pass 'RemoveArgMaxDequantPass' completed. ---                      \n",
      "[INFO] - Running PassManager with 1 rules.                                      \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - --- Pass 'RemoveLinearResizeDequantPass' completed. ---                \n",
      "[INFO] - Running PassManager with 1 rules.                                      \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - --- Pass 'AddRoundAfterAvgPoolPass' completed. ---                     \n",
      "[INFO] - --- All Graph Transformation Passes Finished ---                       \n",
      "[INFO] - --- Running Optimize Stage ---                                         \n",
      "[INFO] - --- Running Node Optimizer Stage ---                                   \n",
      "[INFO] - Running PassManager with 19 rules.                                     \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - --- Running Last Optimize with OptimizeSkipConnectionPass rule ---     \n",
      "[INFO] - Last Optimize finished.                                                \n",
      "[INFO] - Node Optimizer finished. Applied 19 pass(es).                          \n",
      "[INFO] - --- Running Preprocessing Fuser Stage ---                              \n",
      "[INFO] - Skipping preprocessing fusion - not first NPU model or no preprocessings\n",
      "[INFO] - Running PassManager with 1 rules.                                      \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - --- Running Node Optimizer Stage ---                                   \n",
      "[INFO] - Running PassManager with 19 rules.                                     \n",
      "[INFO] - PassManager finished.                                                  \n",
      "[INFO] - --- Running Last Optimize with OptimizeSkipConnectionPass rule ---     \n",
      "[INFO] - Last Optimize finished.                                                \n",
      "[INFO] - Node Optimizer finished. Applied 19 pass(es).                          \n",
      "[INFO] - --- Optimize Stage Finished ---                                        \n",
      "Compiling Model :  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 0.8/1.0 [00:08<00:01,  9.06s/model ]\n",
      "Preparing Frontend IR:   0%|                             | 0/52 [00:00<?, ?op/s]\u001b[A\n",
      "Compiling Model :  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 0.8/1.0 [00:08<00:01,  9.99s/model ]\u001b[A\n",
      "Building layer attributes:   0%|                      | 0/31 [00:00<?, ?layer/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Building connections:   0%|                      | 0/31 [00:00<?, ?connection/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Compiling Model :  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 0.9/1.0 [00:08<00:01, 10.06s/model ]\u001b[A\n",
      "Searching for optimal schedule:  35%|‚ñà‚ñà‚ñà‚ñâ       | 11/31 [00:00<00:00, 51.59it/s]\u001b[A\n",
      "Compiling Model :  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 0.9/1.0 [00:09<00:01, 11.87s/model ]\u001b[A\n",
      "Searching for optimal schedule:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 24/31 [00:00<00:00, 27.33it/s]\u001b[A\n",
      "Searching for optimal schedule:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 28/31 [00:01<00:00, 10.82it/s]\u001b[A\n",
      "Searching for optimal schedule: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:01<00:00, 11.64it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Memory Management:   0%|                           | 0/31 [00:00<?, ?schedule/s]\u001b[A\n",
      "Memory Management:   6%|‚ñà‚ñè                 | 2/31 [00:00<00:01, 16.34schedule/s]\u001b[A\n",
      "Memory Management:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 11/31 [00:00<00:00, 34.42schedule/s]\u001b[A\n",
      "Memory Management:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 19/31 [00:01<00:00, 12.04schedule/s]\u001b[A\n",
      "Memory Management:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/31 [00:02<00:00,  8.98schedule/s]\u001b[A\n",
      "Memory Management:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 25/31 [00:03<00:01,  5.94schedule/s]\u001b[A\n",
      "Memory Management:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 27/31 [00:04<00:00,  4.33schedule/s]\u001b[A\n",
      "Compiling Model :  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 0.9/1.0 [00:14<00:04, 38.41s/model ]\u001b[A\n",
      "Tracking data blocks:   0%|                        | 0/31 [00:00<?, ?schedule/s]\u001b[A\n",
      "Tracking data blocks:   3%|‚ñå               | 1/31 [00:00<00:04,  7.20schedule/s]\u001b[A\n",
      "Tracking data blocks:   6%|‚ñà               | 2/31 [00:00<00:04,  7.08schedule/s]\u001b[A\n",
      "Tracking data blocks:  13%|‚ñà‚ñà              | 4/31 [00:00<00:03,  7.21schedule/s]\u001b[A\n",
      "Tracking data blocks:  26%|‚ñà‚ñà‚ñà‚ñà‚ñè           | 8/31 [00:00<00:01, 12.83schedule/s]\u001b[A\n",
      "Tracking data blocks:  32%|‚ñà‚ñà‚ñà‚ñà‚ñä          | 10/31 [00:00<00:01, 12.84schedule/s]\u001b[A\n",
      "Tracking data blocks:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 12/31 [00:01<00:01, 11.70schedule/s]\u001b[A\n",
      "Tracking data blocks:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 14/31 [00:01<00:02,  7.59schedule/s]\u001b[A\n",
      "Tracking data blocks:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 17/31 [00:01<00:01,  9.06schedule/s]\u001b[A\n",
      "Tracking data blocks:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/31 [00:11<00:17,  1.44s/schedule]\u001b[A\n",
      "Tracking data blocks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 21/31 [00:12<00:11,  1.12s/schedule]\u001b[A\n",
      "Tracking data blocks:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 23/31 [00:14<00:08,  1.09s/schedule]\u001b[A\n",
      "Tracking data blocks:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 25/31 [00:16<00:06,  1.03s/schedule]\u001b[A\n",
      "Tracking data blocks:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 27/31 [00:17<00:04,  1.00s/schedule]\u001b[A\n",
      "Tracking data blocks:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 29/31 [00:19<00:01,  1.14schedule/s]\u001b[A\n",
      "Tracking data blocks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:20<00:00,  1.28schedule/s]\u001b[A\n",
      "Compiling Model :  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 0.9/1.0 [00:35<00:18, 176.32s/model ]\u001b[A\n",
      "Setting DMA Registers:   0%|                       | 0/31 [00:00<?, ?schedule/s]\u001b[A\n",
      "Setting DMA Registers:  29%|‚ñà‚ñà‚ñà‚ñà‚ñé          | 9/31 [00:00<00:01, 12.34schedule/s]\u001b[A\n",
      "Setting DMA Registers:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 13/31 [00:00<00:01, 14.73schedule/s]\u001b[A\n",
      "Setting DMA Registers:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 19/31 [00:01<00:00, 14.84schedule/s]\u001b[A\n",
      "Setting DMA Registers:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 21/31 [00:01<00:00, 12.68schedule/s]\u001b[A\n",
      "Setting DMA Registers:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 23/31 [00:02<00:01,  5.75schedule/s]\u001b[A\n",
      "Setting DMA Registers:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 25/31 [00:03<00:01,  5.68schedule/s]\u001b[A\n",
      "Setting DMA Registers:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 27/31 [00:03<00:00,  5.62schedule/s]\u001b[A\n",
      "Setting DMA Registers:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 29/31 [00:03<00:00,  5.97schedule/s]\u001b[A\n",
      "Setting DMA Registers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:04<00:00,  3.60schedule/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Building RF data:   0%|                            | 0/31 [00:00<?, ?schedule/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Compiling Model :  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1.0/1.0 [00:40<00:03, 115.69s/model ]\u001b[A\n",
      "Compiling Model :  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1.0/1.0 [00:47<00:01, 157.48s/model ]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Saving outputs:   0%|                                  | 0/20 [00:00<?, ?step/s]\u001b[A\n",
      "Creating directories:   0%|                            | 0/20 [00:00<?, ?step/s]\u001b[A\n",
      "Generating SRAM move statistics:   0%|                 | 0/20 [00:00<?, ?step/s]\u001b[A\n",
      "Writing SRAM move stats:   0%|                         | 0/20 [00:00<?, ?step/s]\u001b[A\n",
      "Writing SRAM move figures:   0%|                       | 0/20 [00:00<?, ?step/s]\u001b[A\n",
      "Generating double buffering stats:   0%|               | 0/20 [00:00<?, ?step/s]\u001b[A\n",
      "Writing double buffering stats:   0%|                  | 0/20 [00:00<?, ?step/s]\u001b[A\n",
      "Generating RMAP:   0%|                                 | 0/20 [00:00<?, ?step/s]\u001b[A\n",
      "Generating RMAP:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Writing RMAP:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Generating RMAP info:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Writing RMAP info:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Generating RMAP debug info:  35%|‚ñà‚ñà‚ñà‚ñà‚ñâ         | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Writing RMAP debug info:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Generating RMAP meta info:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Generating simulator info:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Writing simulator info:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Writing compile info:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Generating weights:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Writing weights:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Generating dummy bit mask:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Writing dummy bit mask:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 7/20 [01:11<02:13, 10.26s/step]\u001b[A\n",
      "Compiling Model : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1.0/1.0 [02:02<00:00, 122.39s/model ]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "!./dx_com/dx_com -m clip_image_encoder.onnx -c clip_image.json -o output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5329488-a9d5-49ad-848a-a2255478de40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34moutput\u001b[0m\n",
      "‚îî‚îÄ‚îÄ clip_image_encoder.dxnn\n",
      "\n",
      "1 directory, 1 file\n"
     ]
    }
   ],
   "source": [
    "!tree output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "596a5821-4800-4588-a54c-d693e9aab6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/max/Works/dx-tutorials/dx-all-suite/dx-runtime/dx_rt/python_package\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/max/Works/dx-tutorials/.venv/lib/python3.12/site-packages (from dx-engine==1.1.2) (2.2.6)\n",
      "Building wheels for collected packages: dx-engine\n",
      "  Building wheel for dx-engine (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dx-engine: filename=dx_engine-1.1.2-cp312-cp312-linux_x86_64.whl size=189899 sha256=2989057066156991f3888076a43a6656a6d54d7ab48042dabab11d5ee50b6b96\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uixlx5bt/wheels/1c/bb/97/e08a0a8549cdadcbb71bf23d8fc888c1fc70f706f873c48d66\n",
      "Successfully built dx-engine\n",
      "Installing collected packages: dx-engine\n",
      "Successfully installed dx-engine-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!cd ../../../../dx-runtime/dx_rt/python_package && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "446eca97-d383-47bd-a80d-9f2d2a114878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å!\n",
      "ÏûÖÎ†• ÌååÏùº: cat_sample.jpg\n",
      "ÏµúÏ¢Ö Î∞∞Ïó¥ Shape: (1, 3, 224, 224)\n",
      "ÏµúÏ¢Ö Î∞∞Ïó¥ Dtype: float32\n",
      "Image features extracted. Shape: (1, 512)\n",
      "\n",
      "Cosine Similarity Scores (raw):\n",
      "[[0.24744876 0.24209568 0.2440253  0.2488713 ]]\n",
      "\n",
      "--- Final Probabilities ---\n",
      "a photo of a dog    : 29.00%\n",
      "a photo of a cat    : 16.98%\n",
      "a photo of a bird   : 20.59%\n",
      "a car               : 33.43%\n",
      "\n",
      "‚úÖ Best Match: a car\n",
      "\n",
      "--- ONNX End-to-End Inference Finished ---\n"
     ]
    }
   ],
   "source": [
    "from dx_engine import InferenceEngine\n",
    "import cv2\n",
    "\n",
    "IMAGE_PATH = \"cat_sample.jpg\"\n",
    "IMAGE_RESOLUTION = 224 # \"ViT-B/32\" Î™®Îç∏Ïùò Ìï¥ÏÉÅÎèÑ\n",
    "\n",
    "# CLIP Î™®Îç∏ ÌïôÏäµ Ïãú ÏÇ¨Ïö©Îêú ÌëúÏ§Ä Ï†ïÍ∑úÌôî Í∞í (ÌïÑÏàò)\n",
    "CLIP_MEAN = np.array([0.48145466, 0.4578275, 0.40821073], dtype=np.float32)\n",
    "CLIP_STD = np.array([0.26862954, 0.26130258, 0.27577711], dtype=np.float32)\n",
    "\n",
    "# --- 2. Ïù¥ÎØ∏ÏßÄ Î°úÎìú (OpenCV) ---\n",
    "# cv2.imread()Îäî (H, W, C) ÏàúÏÑúÏù¥Î©∞, BGR ÌòïÏãùÏúºÎ°ú Î°úÎìúÎê©ÎãàÎã§.\n",
    "image = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "if image is None:\n",
    "    print(f\"Ïò§Î•ò: '{IMAGE_PATH}' ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "else:\n",
    "    # --- 3. Ï†ÑÏ≤òÎ¶¨ (BGR -> RGB) ---\n",
    "    # CLIP Î™®Îç∏ÏùÄ RGB ÏûÖÎ†•ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÌïôÏäµÎêòÏóàÏäµÎãàÎã§.\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- 4. Ï†ÑÏ≤òÎ¶¨ (Resize & Center Crop) ---\n",
    "    # clip.preprocessÏùò Resize(interpolation=BICUBIC) Î∞è CenterCrop Ïû¨ÌòÑ\n",
    "    \n",
    "    h, w, _ = image_rgb.shape\n",
    "    \n",
    "    # ÏßßÏùÄ Ï™ΩÏùÑ 224Î°ú ÎßûÏ∂îÍ≥† ÎπÑÏú® Ïú†ÏßÄ\n",
    "    if h < w:\n",
    "        new_h = IMAGE_RESOLUTION\n",
    "        new_w = int(w * (new_h / h)) # ÎπÑÏú®Ïóê ÎßûÍ≤å Í∏¥ Ï™Ω Í≥ÑÏÇ∞\n",
    "    else:\n",
    "        new_w = IMAGE_RESOLUTION\n",
    "        new_h = int(h * (new_w / w))\n",
    "        \n",
    "    # 4-1. Resize (torchvision.transforms.ResizeÏùò BICUBIC -> cv2.INTER_CUBIC)\n",
    "    resized_image = cv2.resize(image_rgb, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # 4-2. Center Crop\n",
    "    top = (new_h - IMAGE_RESOLUTION) // 2\n",
    "    left = (new_w - IMAGE_RESOLUTION) // 2\n",
    "    \n",
    "    cropped_image = resized_image[top:top + IMAGE_RESOLUTION, left:left + IMAGE_RESOLUTION]\n",
    "\n",
    "    # --- 5. Ï†ÑÏ≤òÎ¶¨ (Ï†ïÍ∑úÌôî Î∞è ÌÉÄÏûÖ Î≥ÄÌôò) ---\n",
    "    # HWC, [0, 255] uint8 -> HWC, [0.0, 1.0] float32\n",
    "    image_float = cropped_image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # 5-1. Ï†ïÍ∑úÌôî (Normalize)\n",
    "    normalized_image = (image_float - CLIP_MEAN) / CLIP_STD\n",
    "\n",
    "    # --- 6. Ï†ÑÏ≤òÎ¶¨ (HWC -> CHW) ---\n",
    "    # (224, 224, 3) -> (3, 224, 224)\n",
    "    #image_chw = np.transpose(normalized_image, (2, 0, 1))\n",
    "    image_chw = np.transpose(image_float, (2, 0, 1))\n",
    "\n",
    "    # --- 7. Ï†ÑÏ≤òÎ¶¨ (Î∞∞Ïπò Ï∞®Ïõê Ï∂îÍ∞Ä) ---\n",
    "    # (3, 224, 224) -> (1, 3, 224, 224)\n",
    "    # ONNX Îü∞ÌÉÄÏûÑÏóê ÏûÖÎ†•Ìï† ÏµúÏ¢Ö Î∞∞Ïó¥\n",
    "    image_batch = np.expand_dims(image_chw, axis=0).astype(np.float32)\n",
    "\n",
    "    print(f\"Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å!\")\n",
    "    print(f\"ÏûÖÎ†• ÌååÏùº: {IMAGE_PATH}\")\n",
    "    print(f\"ÏµúÏ¢Ö Î∞∞Ïó¥ Shape: {image_batch.shape}\")\n",
    "    print(f\"ÏµúÏ¢Ö Î∞∞Ïó¥ Dtype: {image_batch.dtype}\")\n",
    "\n",
    "dxnn_model_path = \"output/clip_image_encoder.dxnn\"\n",
    "\n",
    "ie = InferenceEngine(dxnn_model_path)\n",
    "\n",
    "#input = [image_input_np]\n",
    "input = [image_batch]\n",
    "outputs = ie.run(input)\n",
    "#print(outputs)\n",
    "print(f\"Image features extracted. Shape: {outputs[0].shape}\")\n",
    "\n",
    "#image_features_norm = normalize(outputs[0])\n",
    "#text_features_norm = normalize(text_features_np)\n",
    "image_features_norm = normalize(outputs[0])\n",
    "text_features_norm = normalize(text_features_np)\n",
    "\n",
    "# 2. ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ (ÌñâÎ†¨ Í≥±)\n",
    "# (1, 512) @ (512, 4) -> (1, 4)\n",
    "similarity_scores = image_features_norm @ text_features_norm.T\n",
    "print(f\"\\nCosine Similarity Scores (raw):\\n{similarity_scores}\")\n",
    "\n",
    "# 3. ÏµúÏ¢Ö ÌôïÎ•† Í≥ÑÏÇ∞ (logit_scale Ï†ÅÏö© Î∞è softmax)\n",
    "logits = similarity_scores * logit_scale\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(f\"\\n--- Final Probabilities ---\")\n",
    "for label, prob in zip(text_labels, probabilities[0]):\n",
    "    print(f\"{label:<20}: {prob * 100:.2f}%\")\n",
    "\n",
    "# Í≤∞Í≥º ÌôïÏù∏\n",
    "best_match_index = np.argmax(probabilities[0])\n",
    "print(f\"\\n‚úÖ Best Match: {text_labels[best_match_index]}\")\n",
    "\n",
    "print(\"\\n--- ONNX End-to-End Inference Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f019c49b-20c9-4b82-a692-88dc4f9a5864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d288e419144cd2823b056fdd2ca1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from open_clip import tokenizer as clip_tokenizer\n",
    "DEVICE = \"cpu\"  # export Ïö©ÏúºÎ°† CPU Í∂åÏû•\n",
    "MODEL_NAME = \"ViT-L-14-quickgelu\"\n",
    "PRETRAINED = \"dfn2b\"   # ÌïÑÏöî Ïãú ÏõêÌïòÎäî ÌÉúÍ∑∏Î°ú ÍµêÏ≤¥\n",
    "# 1) Î™®Îç∏/Ï†ÑÏ≤òÎ¶¨ Î°úÎìú (Í∞ÄÏ§ëÏπò ÏûêÎèô Îã§Ïö¥Î°úÎìú & Ï∫êÏãú)\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    MODEL_NAME, pretrained=PRETRAINED, device=DEVICE\n",
    ")\n",
    "model.eval()\n",
    "# 2) Ïù¥ÎØ∏ÏßÄ/ÌÖçÏä§Ìä∏ Ïù∏ÏΩîÎçî ÎûòÌçº Ï†ïÏùò (ÌîÑÎ°úÏ†ùÏÖò Ìè¨Ìï®, L2 Ï†ïÍ∑úÌôî ÎπÑÌôúÏÑ±Ìôî)\n",
    "class ImageEncoder(torch.nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.visual = clip_model.visual\n",
    "        self.image_proj = getattr(clip_model, \"image_projection\", None)\n",
    "        self.normalize = torch.nn.functional.normalize\n",
    "    def forward(self, x):\n",
    "        # x: [1,3,H,W] (preprocessÎ°ú Ï†ïÍ∑úÌôîÎêú ÌÖêÏÑú)\n",
    "        feats = self.visual(x)                      # pooled visual features\n",
    "        if self.image_proj is not None:\n",
    "            feats = feats @ self.image_proj         # projection to embed_dim\n",
    "        # Î∞∞Ìè¨ ÌååÏù¥ÌîÑÎùºÏù∏ÏóêÏÑú Î≥¥ÌÜµ ÌõÑÏ≤òÎ¶¨Î°ú L2 Ï†ïÍ∑úÌôîÌïòÎØÄÎ°ú ONNXÏóî ÎπÑÏ†ïÍ∑úÌôî Ï∂úÎ†•\n",
    "        return feats\n",
    "class TextEncoder(torch.nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.text = clip_model.transformer\n",
    "        self.text_projection = getattr(clip_model, \"text_projection\", None)\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.token_embedding = clip_model.token_embedding\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.register_buffer(\"attn_mask\", clip_model.attn_mask)\n",
    "    def forward(self, tokens):\n",
    "        # tokens: [1, 77] (int32/int64)\n",
    "        x = self.token_embedding(tokens) + self.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.text(x, attn_mask=self.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x)\n",
    "        # EOT ÌÜ†ÌÅ∞Ïùò ÏûÑÎ≤†Îî© ÏÑ†ÌÉù (OpenAI CLIP Í∑úÏïΩ)\n",
    "        eot_indices = tokens.argmax(dim=-1)\n",
    "        x = x[torch.arange(x.shape[0]), eot_indices]  # [N, hidden]\n",
    "        if self.text_projection is not None:\n",
    "            x = x @ self.text_projection\n",
    "        return x\n",
    "img_encoder = ImageEncoder(model).to(DEVICE).eval()\n",
    "txt_encoder = TextEncoder(model).to(DEVICE).eval()\n",
    "# 3) ÎçîÎØ∏ ÏûÖÎ†• (Î∞∞Ïπò=1 Í≥†Ï†ï)\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞: Î™®Îç∏Ïóê ÎÇ¥Ïû•Îêú ÏûÖÎ†• Ìï¥ÏÉÅÎèÑ ÏÇ¨Ïö©\n",
    "image_size = getattr(model.visual, \"image_size\", 224)\n",
    "dummy_image = torch.randn(1, 3, image_size, image_size, device=DEVICE)\n",
    "dummy_tokens = clip_tokenizer.tokenize([\"dummy text\"]).to(DEVICE)  # [1,77]\n",
    "# 4) ONNX ÎÇ¥Î≥¥ÎÇ¥Í∏∞\n",
    "common = dict(\n",
    "    opset_version=17,          # 17 Ïù¥ÏÉÅ Í∂åÏû•\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes=None          # Î∞∞Ïπò=1 Í≥†Ï†ï\n",
    ")\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        img_encoder, dummy_image,\n",
    "        f\"clip_{MODEL_NAME}_image_{PRETRAINED}.onnx\",\n",
    "        input_names=[\"pixel_values\"],\n",
    "        output_names=[\"image_embeds\"],\n",
    "        **common\n",
    "    )\n",
    "    torch.onnx.export(\n",
    "        txt_encoder, dummy_tokens,\n",
    "        f\"clip_{MODEL_NAME}_text_{PRETRAINED}.onnx\",\n",
    "        input_names=[\"input_ids\"],\n",
    "        output_names=[\"text_embeds\"],\n",
    "        **common\n",
    "    )\n",
    "print(\"DONE :Ìù∞ÏÉâ_ÌôïÏù∏_ÌëúÏãú:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863da92-268b-472a-87e2-4f9b741856fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building zero-shot classifier weights on 'cpu'...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import open_clip\n",
    "from open_clip.zero_shot_classifier import build_zero_shot_classifier\n",
    "\n",
    "def import_imagenet_metadata():\n",
    "    try:\n",
    "        from open_clip.zero_shot_metadata import (\n",
    "            IMAGENET_CLASSNAMES,\n",
    "            OPENAI_IMAGENET_TEMPLATES,\n",
    "        )\n",
    "        return IMAGENET_CLASSNAMES, OPENAI_IMAGENET_TEMPLATES\n",
    "    except ImportError:\n",
    "        print(\"\\n[ERROR] Could not import IMAGENET_CLASSNAMES or OPENAI_IMAGENET_TEMPLATES.\")\n",
    "        print(\"This suggests a mismatch between your source code and the installed library in your conda environment.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "device = 'cpu'\n",
    "model_name = 'ViT-L-14-quickgelu'\n",
    "pre_trained = 'dfn2b'\n",
    "output_path = os.path.join('./', f\"{model_name}-{pre_trained}.npy\")\n",
    "\n",
    "# Import classnames and templates\n",
    "classnames, templates = import_imagenet_metadata()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name, pretrained=pre_trained, device=device\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Build zero-shot classifier weights\n",
    "print(f\"Building zero-shot classifier weights on '{device}'...\")\n",
    "with torch.no_grad():\n",
    "    zeroshot_weights = build_zero_shot_classifier(\n",
    "        model,\n",
    "        tokenizer=tokenizer,\n",
    "        classnames=classnames,\n",
    "        templates=templates,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "print(f\"Zero-shot weights shape: {zeroshot_weights.shape}, dtype: {zeroshot_weights.dtype}\")\n",
    "\n",
    "# Save as numpy\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "np.save(output_path, zeroshot_weights.cpu().numpy())\n",
    "print(f\"Zero-shot weights saved to: {output_path}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3380b624-3a2a-4ae4-b62c-5acf7e6ac4be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
