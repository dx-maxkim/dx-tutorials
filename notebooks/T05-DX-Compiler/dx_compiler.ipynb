{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71513b35-9e60-4c49-b3ac-45c66d7f3933",
   "metadata": {},
   "source": [
    "# DEEPX Tutorial 05 - DX-Compiler Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38306cf-27f3-454b-9794-eb4ce4ee76a3",
   "metadata": {},
   "source": [
    "In Tutorial 5, you will practice compiling classification, object detection, and segmentation models using the DX-Compiler. We will also review the guide for troubleshooting problems during compilation.\n",
    "\n",
    "For more details, refer to the DX-Compiler user guide ðŸ‘‰ [Download](https://developer.deepx.ai/?files=MjY0NA==)\n",
    "\n",
    ">This tutorial is based on dx-all-suite v2.1.0, released in December 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3d09f-4ccd-4589-8c7a-aa58cfd510fc",
   "metadata": {},
   "source": [
    "## Compiling Image Classification Model (MobileNetV2)\n",
    "1. Export PyTorch â†’ ONNX\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cbee8-6cbc-4a78-8700-1cdc65076c4d",
   "metadata": {},
   "source": [
    "### 1. Export PyTorch â†’ ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c2b5de9-ab8a-466e-bfba-823f0fe96817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch & onnx\n",
    "!pip install --quiet torch torchvision onnx onnxsim onnxscript netron portpicker tqdm seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87986a8c-af2b-43f5-8860-c63f0bc14ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639e9cb-7a79-422a-93e4-700ad8fcf472",
   "metadata": {},
   "source": [
    "Export Pytorch based MobileNetV2 model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e609d-ba12-45e5-8f40-f0d443165d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "# Load MobileNetV2 model\n",
    "model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Batch size must be 1\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "onnx_path = \"MobilenetV2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                      # PyTorch model object to export\n",
    "    dummy_input,                # Dummy input used for tracing (tuple is possible)\n",
    "    onnx_path,                  # Output ONNX file path\n",
    "    export_params=True,         # If True, saves model parameter (weight) into the ONNX file\n",
    "    input_names=[\"input_test\"], # Name of the ONNX model input tensor\n",
    "    output_names=[\"output\"],    # Name of the ONNX model output tensor\n",
    "    opset_version=13            # ONNX opset version (recommended: 11 ~ 21)\n",
    ")\n",
    "print(\"âœ… Save ONNX:\", onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78759017-3ffe-495f-be9a-337ac8fd0552",
   "metadata": {},
   "source": [
    "Clarify the **Input Name** and **Input Shape** using netron which see the AI model's topology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d068300-5fe1-4b30-9388-8b621a681383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "import os\n",
    "\n",
    "# 1. Set port\n",
    "port = 8081\n",
    "\n",
    "# 2. Start Netron Server\n",
    "netron.start('MobilenetV2.onnx', address=('localhost', port), browse=False)\n",
    "\n",
    "# 3. Create Proxy URL\n",
    "proxy_url = f\"/proxy/{port}/\" \n",
    "\n",
    "# 4. Display IFrame\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=proxy_url, width='100%', height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435e445-e87e-4706-ad8f-09d040477cb0",
   "metadata": {},
   "source": [
    "### 2. JSON configuration for Input/Pre-processing/Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e037ae0-6ebd-4429-a5a6-70d56c9f818c",
   "metadata": {},
   "source": [
    "Generate a configuration file for Input/Pre-processing/Calibration of MobilenetV2.\n",
    "\n",
    "This JSON configuration file incudes:\n",
    " - Input specifications\n",
    " - Calibration methods\n",
    " - Data preprocessing settings\n",
    " - Optional parameters for advanced compilation schemes\n",
    "\n",
    "Model **Input Restrictions**:\n",
    " - The batch size must be fixed to 1\n",
    " - Only a single input is supported (Multi-input will be supported in 2026)\n",
    " - Input name must exactly match ONNX model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e10ab-7804-4c31-b4d0-e39dd16749d9",
   "metadata": {},
   "source": [
    "#### 2.1. Incorrect input name case\n",
    "\n",
    "This MobilenetV2 model's input name is `input_test` as shown below:\n",
    "\n",
    "![](assets/mobilenetv2-input-name.png)\n",
    "\n",
    "Input name (`input_test` in this example) must exactly match the input name of JSON configuration.\n",
    "\n",
    "However, you can see there is wrong input name `incorrect_input_name` in the following JSON configuration for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe57234-a230-4925-be56-b3a410f26ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"incorrect_input_name\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47876a-1379-4bce-be6c-6889a0cb1397",
   "metadata": {},
   "source": [
    "Let's run DX-Compiler with this wrong JSON configuration. You will meet ERROR:\n",
    "> ConfigInputError: The input name in config incorrect_input_name is not same as model input input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a8e81-84d6-4e3d-93ba-e7026b4282cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01180d0a-2bfc-4ef8-8ed0-be93cbc8f755",
   "metadata": {},
   "source": [
    "#### 2.2. Incorrect input shape\n",
    "In the following JSON file, the input name has the correct one - `input_test`.\n",
    "\n",
    "However, even if your input shape is 1x3x224x224 (BxCxHxW), calibration image shape has 224x224x3 (HxWxC). You must change the shape of calibraiton image to match with 1x3x224x224 (BxCxHxW) by using `transpose` and `expandDim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655c076-5ac0-4049-bac6-b49ec98dce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"input_test\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc83888-67b8-4a2a-a8f2-b52787d2766e",
   "metadata": {},
   "source": [
    "Let's run DX-COM with this wrong JSON configuration. You will meet ERROR:\n",
    "\n",
    "> ConfigInputError: Config shape [1, 3, 224, 224] does not match preprocessed data shape [1, 224, 224, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ac0f7-0b8e-4dce-b9be-d31586657888",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57ef8e-0d99-425a-85c7-390480494f4c",
   "metadata": {},
   "source": [
    "#### 2.3. Add `transpose` & `expanDim` to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bbd86f-e2c9-4aef-8c4d-96ed3974caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"input_test\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cd446-8d2e-4c74-98ba-1e481eb57cfc",
   "metadata": {},
   "source": [
    "### 3. Compile with DX-Compiler and verify .dxnn\n",
    "Compile with `dx_com` to generate `.dxnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2270e5a-6b04-457b-a7b1-33a3948515d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2907f72-5dc2-435b-9dc5-7b319a8bd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m MobilenetV2.dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30a3ea-3335-4809-9b65-e0ea0479db29",
   "metadata": {},
   "source": [
    "## Compiling Object Detection Model (YOLOv9s)\n",
    "1. Export PyTorch â†’ ONNX\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133bb6f-807e-43ba-8fa9-1c55b5691280",
   "metadata": {},
   "source": [
    "### 1. Export PyTorch â†’ ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b15e4a-d1dd-4968-a046-e5217b12972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/max/Works/dx-tutorials/dx-all-suite/workspace/release/dx_com/dx_com_M1_v2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3dbaa-e9b2-48ea-aacf-b08f7f26124b",
   "metadata": {},
   "source": [
    "#### 1.1. Download `yolov9-s.pt` pytorch model to your local system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cfb8d-1f8e-4e70-8d2a-28dfaff0c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd yolov9 && pip install -r requirements.txt\n",
    "!pip install torch==2.5.1 torchvision==0.20.1\n",
    "!pip install onnx onnxsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80ae6a3d-d15c-4861-9794-dedd0ad4aeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-12 16:08:01--  https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-s.pt\n",
      "Resolving github.com (github.com)... 20.200.245.247\n",
      "Connecting to github.com (github.com)|20.200.245.247|:443... connected.\n",
      "WARNING: cannot verify github.com's certificate, issued by â€˜emailAddress=support@fortinet.com,CN=FG100FTK20044476,OU=Certificate Authority,O=Fortinet,L=Sunnyvale,ST=California,C=USâ€™:\n",
      "  Self-signed certificate encountered.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/759338070/5efccc50-06db-4a1f-bf78-db9fcce17d09?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-12T07%3A44%3A31Z&rscd=attachment%3B+filename%3Dyolov9-s.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-12T06%3A44%3A20Z&ske=2025-12-12T07%3A44%3A31Z&sks=b&skv=2018-11-09&sig=GXmOTdnUDBuzu7oXjo19cKfxeJefLnKJ0v8GWidgnvQ%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTUyNTA4MiwibmJmIjoxNzY1NTIzMjgyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.GV6E84i4Cj7QAmjREW2bkVb2QtssczKY_z1cqi0NF2s&response-content-disposition=attachment%3B%20filename%3Dyolov9-s.pt&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-12-12 16:08:02--  https://release-assets.githubusercontent.com/github-production-release-asset/759338070/5efccc50-06db-4a1f-bf78-db9fcce17d09?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-12T07%3A44%3A31Z&rscd=attachment%3B+filename%3Dyolov9-s.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-12T06%3A44%3A20Z&ske=2025-12-12T07%3A44%3A31Z&sks=b&skv=2018-11-09&sig=GXmOTdnUDBuzu7oXjo19cKfxeJefLnKJ0v8GWidgnvQ%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTUyNTA4MiwibmJmIjoxNzY1NTIzMjgyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.GV6E84i4Cj7QAmjREW2bkVb2QtssczKY_z1cqi0NF2s&response-content-disposition=attachment%3B%20filename%3Dyolov9-s.pt&response-content-type=application%2Foctet-stream\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20423912 (19M) [application/octet-stream]\n",
      "Saving to: â€˜yolov9-s.ptâ€™\n",
      "\n",
      "yolov9-s.pt         100%[===================>]  19.48M  9.85MB/s    in 2.0s    \n",
      "\n",
      "2025-12-12 16:08:04 (9.85 MB/s) - â€˜yolov9-s.ptâ€™ saved [20423912/20423912]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-s.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218e930-06a4-4be6-bf02-80f64c150f34",
   "metadata": {},
   "source": [
    "#### 1.2. Export the downloaded torch model to ONNX:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c0d0b-c1d9-4128-8c0a-afc36fbcf020",
   "metadata": {},
   "source": [
    "Download yolov9 git repo to use `export.py` for yolov9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0581e7e5-8d01-4bea-b11b-a59d2de35968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov9'...\n",
      "remote: Enumerating objects: 781, done.\u001b[K\n",
      "remote: Total 781 (delta 0), reused 0 (delta 0), pack-reused 781 (from 1)\u001b[K\n",
      "Receiving objects: 100% (781/781), 3.27 MiB | 23.25 MiB/s, done.\n",
      "Resolving deltas: 100% (330/330), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov9.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4799a-bd6a-4551-bded-724390318036",
   "metadata": {},
   "source": [
    "Export Pytorch based `yolov9-s` model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34a9d87a-9ce4-40c5-8722-758e356e817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/max/Works/dx-tutorials/dx-all-suite/workspace/release/dx_com/dx_com_M1_v2.1.0/yolov9/utils/general.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources as pkg\n",
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco.yaml, weights=['../yolov9-s.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLO ðŸš€ v0.1-104-g5b1ea9a Python-3.12.3 torch-2.9.1+cu128 CPU\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/max/Works/dx-tutorials/dx-all-suite/workspace/release/dx_com/dx_com_M1_v2.1.0/yolov9/export.py\", line 686, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/max/Works/dx-tutorials/dx-all-suite/workspace/release/dx_com/dx_com_M1_v2.1.0/yolov9/export.py\", line 681, in main\n",
      "    run(**vars(opt))\n",
      "  File \"/home/max/Works/dx-tutorials/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max/Works/dx-tutorials/dx-all-suite/workspace/release/dx_com/dx_com_M1_v2.1.0/yolov9/export.py\", line 544, in run\n",
      "    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max/Works/dx-tutorials/dx-all-suite/workspace/release/dx_com/dx_com_M1_v2.1.0/yolov9/models/experimental.py\", line 243, in attempt_load\n",
      "    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max/Works/dx-tutorials/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 1529, in load\n",
      "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
      "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL models.yolo.DetectionModel was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.yolo.DetectionModel])` or the `torch.serialization.safe_globals([models.yolo.DetectionModel])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "!cd yolov9 && python3 export.py --weights ../yolov9-s.pt \\\n",
    "                                --img-size 640 640 \\\n",
    "                                --opset 12 \\\n",
    "                                --simplify \\\n",
    "                                --batch-size 1 \\\n",
    "                                --include onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee5d9f-7328-49a2-bfc5-77eef6ac9b44",
   "metadata": {},
   "source": [
    "Check if `yolov9-s.onnx` file is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "278d853b-16f6-4d36-8419-582d7ec034b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls | grep yolov9-s.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4186075-a2ec-4147-80a7-bac3ebe3608d",
   "metadata": {},
   "source": [
    "### 2. JSON configuration for Input/Pre-processing/Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed9c6bc5-01a4-443b-9a0c-4e0ea048acd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing yolov9-s.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile yolov9-s.json\n",
    "{\n",
    "  \"inputs\": {\"images\": [1,3,640,640]},\n",
    "  \"calibration_num\": 100,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"div\": {\"x\": 255}},\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bd2bb-83b2-4646-80ec-f0aa528bbe61",
   "metadata": {},
   "source": [
    "### 3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a56efd1b-4e30-436a-87ff-96f6e2aef06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Using optimization level 1. Compilation may take longer.\n",
      "[INFO] - For faster compilation, consider using --opt_level 0 (may increase NPU latency).\n",
      "[INFO] - Starting file validation...\n",
      "[ERROR] - ========================================================================================================================\n",
      "[ERROR] - Error in PhaseLabels.PREPARE: ONNXFileNotFoundError ONNX model file not found: yolov9-s.onnx\n",
      "[ERROR] - ========================================================================================================================\n",
      "Failed to compile yolov9-s.onnx.\n",
      "Error Log:\n",
      "\tONNXFileNotFoundError: ONNX model file not found: yolov9-s.onnx\n"
     ]
    }
   ],
   "source": [
    "!./dx_com/dx_com -m yolov9-s.onnx -c yolov9-s.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdea9f5-9c9a-42f5-a397-3b1d6a2be817",
   "metadata": {},
   "source": [
    "Check if **yolov9-s.dxnn** file is generated as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9074fe4-a5b0-44e9-91f7-af519106e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls | grep yolov9-s.dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f2f50a-5fe9-49af-9cb9-bac6fe82eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m yolov9-s.dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e1b99-96ca-412e-98bc-304e57c6c6bf",
   "metadata": {},
   "source": [
    "## Compiling ViT Model (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca25b64-3bfd-4c60-86a7-84a3c240618a",
   "metadata": {},
   "source": [
    "https://github.com/mlfoundations/open_clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85b1f2-8425-4a6a-ab36-d7de489c887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ff1ab-cb41-4335-b0de-750737d9b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q onnxruntime onnxsim onnx onnxscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e4afe-1533-4e3b-bc41-9a33158a3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import open_clip\n",
    "from open_clip.zero_shot_classifier import build_zero_shot_classifier\n",
    "\n",
    "def import_imagenet_metadata():\n",
    "    try:\n",
    "        from open_clip.zero_shot_metadata import (\n",
    "            IMAGENET_CLASSNAMES,\n",
    "            OPENAI_IMAGENET_TEMPLATES,\n",
    "        )\n",
    "        return IMAGENET_CLASSNAMES, OPENAI_IMAGENET_TEMPLATES\n",
    "    except ImportError:\n",
    "        print(\"\\n[ERROR] Could not import IMAGENET_CLASSNAMES or OPENAI_IMAGENET_TEMPLATES.\")\n",
    "        print(\"This suggests a mismatch between your source code and the installed library in your conda environment.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def main():\n",
    "    device = 'cuda'#'cpu'\n",
    "    # Model              | Pre-trained\n",
    "    # ----------------------------------------\n",
    "    # ViT-B-32-quickgelu | metaclip_fullcc\n",
    "    # ViT-B-16-quickgelu | metaclip_fullcc\n",
    "    # ViT-L-14-quickgelu | dfn2b\n",
    "    # ViT-B-16           | dfn2b\n",
    "    # ViT-L-14           | datacomp_xl_s13b_b90k\n",
    "    # ViT-B-32-256       | datacomp_s34b_b86k\n",
    "    # ViT-L-14-336       | openai\n",
    "    model_name = 'ViT-L-14-quickgelu'\n",
    "    pretrained = 'dfn2b'\n",
    "    output_dir = 'out'\n",
    "    output_path = os.path.join(output_dir, f\"{model_name}-{pretrained}.npy\")\n",
    "\n",
    "    # Import classnames and templates\n",
    "    classnames, templates = import_imagenet_metadata()\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(f\"Loading model '{model_name}' ({pretrained}) on '{device}'...\")\n",
    "    model, _, _ = open_clip.create_model_and_transforms(\n",
    "        model_name, pretrained=pretrained, device=device\n",
    "    )\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Build zero-shot classifier weights\n",
    "    print(f\"Building zero-shot classifier weights on '{device}'...\")\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = build_zero_shot_classifier(\n",
    "            model,\n",
    "            tokenizer=tokenizer,\n",
    "            classnames=classnames,\n",
    "            templates=templates,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    print(f\"Zero-shot weights shape: {zeroshot_weights.shape}, dtype: {zeroshot_weights.dtype}\")\n",
    "\n",
    "    # Save as numpy\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.save(output_path, zeroshot_weights.cpu().numpy())\n",
    "    print(f\"Zero-shot weights saved to: {output_path}\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24dafc-4dd7-4974-9a6a-1aa1d43a123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91c732-b5b6-448b-9ae5-5812950a89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7f110-5a4b-425d-8350-7dd1613e2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxsim\n",
    "from PIL import Image\n",
    "\n",
    "def parse_preprocess(preprocess):\n",
    "    dx_preprocess = []\n",
    "\n",
    "    for trf in preprocess.transforms:\n",
    "        trf_name = trf.__class__.__name__\n",
    "\n",
    "        if trf_name == \"Resize\":\n",
    "            size = trf.size\n",
    "            if isinstance(size, int):\n",
    "                dx_preprocess.append({\"resize\": {\"width\": size, \"height\": size}})\n",
    "            else:\n",
    "                dx_preprocess.append({\"resize\": {\"width\": size[0], \"height\": size[1]}})\n",
    "        elif trf_name == \"ToTensor\":\n",
    "            dx_preprocess.append({\"div\": {\"x\": 255.0}})\n",
    "        elif trf_name == \"Normalize\":\n",
    "            mean = trf.mean\n",
    "            std = trf.std\n",
    "            dx_preprocess.append(\n",
    "                {\"normalize\": {\"mean\": mean, \"std\": std}},\n",
    "            )\n",
    "        elif trf_name == \"function\":\n",
    "            trf_name = trf.__name__\n",
    "            if trf_name == \"_convert_to_rgb\":\n",
    "                dx_preprocess.append({\"convertColor\": {\"form\": \"BGR2RGB\"}})\n",
    "            else:\n",
    "                raise NotImplementedError(trf_name)\n",
    "        elif trf_name == \"function\":\n",
    "            trf_name = trf.__name__\n",
    "            if trf_name == \"_convert_to_rgb\":\n",
    "                dx_preprocess.append({\"convertColor\": {\"form\": \"BGR2RGB\"}})\n",
    "            else:\n",
    "                raise NotImplementedError(trf_name)\n",
    "\n",
    "        elif trf_name == \"CenterCrop\":\n",
    "            size = trf.size\n",
    "\n",
    "            if isinstance(size, int):\n",
    "                dx_preprocess.append({\"centercrop\": {\"width\": size, \"height\": size}})\n",
    "            else:\n",
    "                dx_preprocess.append({\"centercrop\": {\"width\": size[0], \"height\": size[1]}})\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(trf_name)\n",
    "    return dx_preprocess\n",
    "\n",
    "def get_config(img: torch.Tensor, preprocess):\n",
    "    template = {\n",
    "        \"inputs\": {\"input\": [1, 3, 224, 224]},\n",
    "        \"default_loader\": {\n",
    "            \"dataset_path\": \"/mnt/datasets/calibration_dataset/\",\n",
    "            \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "        },\n",
    "        \"calibration_num\": 100,\n",
    "        \"calibration_method\": \"ema\",\n",
    "        \"train_batchsize\": 32,\n",
    "        \"num_samples\": 100,\n",
    "    }\n",
    "    template[\"inputs\"][\"input\"] = img.shape\n",
    "    _preprocess = parse_preprocess(preprocess)\n",
    "    _preprocess.append({\"transpose\": {\"axis\": [2, 0, 1]}})\n",
    "    _preprocess.append({\"expandDim\": {\"axis\": 0}})\n",
    "\n",
    "    template[\"default_loader\"][\"preprocessings\"] = _preprocess\n",
    "    return template\n",
    "\n",
    "def main():\n",
    "    output_dir = 'out'\n",
    "    model_name = 'ViT-L-14-quickgelu'\n",
    "    pretrained = 'dfn2b'\n",
    "    output_path = os.path.join(output_dir, f\"{model_name}-{pretrained}.onnx\")\n",
    "    config_path = os.path.join(output_dir, f\"{model_name}-{pretrained}.json\")\n",
    "    transform_path = os.path.join(output_dir, f\"{model_name}-{pretrained}-transform.txt\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(f\"Loading model '{model_name}' ({pretrained}) ...\")\n",
    "    model, _, transform = open_clip.create_model_and_transforms(\n",
    "        model_name, pretrained=pretrained\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    image = transform(Image.open(\"assets/CLIP.png\")).unsqueeze(0)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    torch.onnx.export(model.visual, image, output_path, opset_version=17)\n",
    "    with open(transform_path, \"w\") as f:\n",
    "        f.write(str(transform))\n",
    "    print(f\"ONNX model saved to: {output_path}\")\n",
    "    config = get_config(image, transform)\n",
    "    with open(config_path, \"w\") as f:\n",
    "        import json\n",
    "        json.dump(config, f)\n",
    "    print(f\"Config file saved to: {config_path}\")\n",
    "\n",
    "    print(\"Simplifying ONNX model ...\")\n",
    "    onnx_model = onnxsim.simplify(output_path)[0]\n",
    "    onnx.save(onnx_model, output_path)\n",
    "    print(f\"Simplified ONNX model saved to: {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5810d22-3017-41a0-928d-e0367c249688",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e82ca-c93a-4ed2-8bf6-25bbd585e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "./dx_com/dx_com -m sample/ViT-L-14-quickgelu-dfn2b.onnx -c sample/ViT-L-14-quickgelu-dfn2b.json  -o out\n",
    "Compiling Model : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0/1.0 [01:17<00:00, 43.33s/model ]Failed to compile sample/ViT-L-14-quickgelu-dfn2b.onnx.\n",
    "Error Log:\n",
    "\tTypeError: Sequential.__init__() takes 2 positional arguments but 3 were given"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
