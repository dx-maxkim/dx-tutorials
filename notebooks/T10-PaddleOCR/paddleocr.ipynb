{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0732a2-6cad-4379-b9d2-4b05600bee9f",
   "metadata": {},
   "source": [
    "# DEEPX Tutorial 10 - PaddleOCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95faca-1f3f-4c8b-b23f-0f41f667f241",
   "metadata": {},
   "source": [
    "This tutorial intruduces basic concept of PaddleOCR and how to use **PP-OCRv5** with DEEPX NPU.\n",
    "\n",
    "**What You Will Learn**:\n",
    " - Understand what PaddleOCR is and basic AI pipeline for OCR\n",
    " - Overall workflow to use PaddleOCR on DEEPX NPU\n",
    " - How to improve OCR accuracy\n",
    "\n",
    ">This tutorial is based on dx-all-suite v2.1.0, released in December 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8a328-288d-489d-8796-79dd0caeacb5",
   "metadata": {},
   "source": [
    "## 1. What is OCR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ce251-51b3-4f9b-bbcc-15ae662182bf",
   "metadata": {},
   "source": [
    "**Optical Character Recognition** (OCR) is the technology that converts different types of documents (scanned paper documents, PDF files, or images captured by a digital camera) into editable and searchable data.\n",
    "\n",
    "Think of it as giving \"eyes\" to your AI. It generally works in a two-step pipeline:\n",
    "1. Text Detection: Locating where the text is in an image (drawing a box around it).\n",
    "2. Text Recognition: Deciphering what the characters inside that box are.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*2hxwOTzkZQh6EDJDPj4_xg.png\" style=\"max-width: 1000px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a7eb0-558d-43ee-bcce-a0efe6a50414",
   "metadata": {},
   "source": [
    "## 2. PaddleOCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c543803-75f1-49af-8b73-5ae6e56cd222",
   "metadata": {},
   "source": [
    "PaddleOCR is an ultra-lightweight, open-source OCR system developed by Baidu based on the PaddlePaddle framework.\n",
    "- PP-OCRv5 (The Latest): The v5 release is optimized for even higher accuracy and speed compared to previous versions.\n",
    "- Key Advantages:\n",
    "  * Lightweight: It offers \"server\" models (high accuracy) and \"mobile\" models (tiny size, perfect for NPUs).\n",
    "  * Multilingual: Supports over 80 languages.\n",
    "  * Rich Toolset: Includes high-quality tools for data annotation and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029489d-cbf8-4c71-86db-80dfbc55d5b7",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg\" style=\"max-width: 1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b0c5f-d675-43a4-b2f7-8adaced87c55",
   "metadata": {},
   "source": [
    "## 3. AI workflow to use PaddleOCRv5 for DEEPX NPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72919348-8579-42e7-927e-be98a62dd364",
   "metadata": {},
   "source": [
    "<img src=\"assets/ocr-workflow.jpg\" style=\"max-width: 1000px;\">\n",
    "\n",
    "To apply PaddleOCR to DX NPU, following 4 steps are required:\n",
    "\n",
    "1. Download PaddleOCR ONNX model\n",
    "\n",
    "2. Fix the dynamic input shape\n",
    "\n",
    "3. Compile ONNX to *.dxnn for DX NPU\n",
    "\n",
    "4. Implement OCR application with DEEPX-SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2b5a7-cbb1-44c1-a576-06bed8a7e360",
   "metadata": {},
   "source": [
    "## 4. Step #1 - Download PaddleOCR ONNX models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434265eb-2e92-4fd5-b7a6-a4573da4cea0",
   "metadata": {},
   "source": [
    "PaddleOCRv5 uses three AI models internally as shown below:\n",
    "1. **DET**: PP-OCRv5_det - text detection\n",
    "2. **CLS**: Text Line Orientation Classification\n",
    "3. **REC**: PP-OCRv5_rec - text recognition\n",
    "\n",
    "You can download following three ONNX models.\n",
    "\n",
    "<img src=\"assets/algorithm_ppocrv5.png\" style=\"max-width: 1000px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384b041-c0ba-49b0-af87-5adc035e61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to models path\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/notebooks/T10-PaddleOCR/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f1652-1f7b-48a4-b222-cdde1ec0941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/jingsongliujing/OnnxOCR\n",
    "\n",
    "# Download 'det' model\n",
    "!wget --no-check-certificate \"https://github.com/jingsongliujing/OnnxOCR/raw/refs/heads/main/onnxocr/models/ppocrv5/det/det.onnx\"\n",
    "\n",
    "# Download 'cls' model\n",
    "!wget --no-check-certificate \"https://github.com/jingsongliujing/OnnxOCR/raw/refs/heads/main/onnxocr/models/ppocrv5/cls/cls.onnx\"\n",
    "\n",
    "# Download 'rec' model\n",
    "!wget --no-check-certificate \"https://github.com/jingsongliujing/OnnxOCR/raw/refs/heads/main/onnxocr/models/ppocrv5/rec/rec.onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc68f24-1be3-4ccf-8bc9-c424616514e7",
   "metadata": {},
   "source": [
    "## 5. Step #2 - Fix the Dynamic Input Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0155347-c7b7-41ca-98b8-433cd3bc4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230062d0-5729-498f-a0e9-7c1ebdd1f0f2",
   "metadata": {},
   "source": [
    "### 5.1. You can check the input shape of each downloaded ONNX model at [netron.app](https://netron.app/) or using python code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341e209-d588-4dc7-b100-9d8e97d0af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "def print_input_shape(model_path):\n",
    "    session = ort.InferenceSession(model_path)\n",
    "    print(f\"---- Input Shapes of {model_path} ----\")\n",
    "    for input_meta in session.get_inputs():\n",
    "        print(f\"Name: {input_meta.name}, Shape: {input_meta.shape}, Type: {input_meta.type}\")\n",
    "        print(\" \" * 30)\n",
    "\n",
    "print_input_shape(\"det.onnx\")\n",
    "print_input_shape(\"cls.onnx\")\n",
    "print_input_shape(\"rec.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57b32fc-ce1c-415f-8760-3fb6de72621b",
   "metadata": {},
   "source": [
    "As you see, there are dynamic input shapes for `[Batch, Channel, Height, Width]` and those dynamic input shape should be static for DEEPX NPU.\n",
    "\n",
    "`ONNX Simplier` is highly recommeded way to fix dynamic shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b8cb3-eaf6-485f-a8a4-9db795ef4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ONNX Simplier\n",
    "!pip install -q onnxsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627ae54-8e43-485b-95e6-4f7e10b3f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '--overwrite-input-shape' option to make dynamic input shape to static one\n",
    "!onnxsim -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74479a0-2851-4125-afa4-ef81d7a8450c",
   "metadata": {},
   "source": [
    "### 5.2. Fix input shape of TEXT Detection Model\n",
    "Recommeded width/heidght range is 320 to 1280 depending on your use case. In this tutorial, 480x480 will be used.\n",
    "> Note: it must be **Multiple of 32**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a22fc3-d2ed-4187-aceb-b96bd352e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!onnxsim det.onnx det_fixed.onnx --overwrite-input-shape \"x:1,3,480,480\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cbd00a-ae49-47f9-9eaf-4c9efc2d58fa",
   "metadata": {},
   "source": [
    "### 5.3. Fix input shape of Classification Model\n",
    "Recommeded width/heidght range is 48 to 192 depending on your use case. These values are the standard of PaddleOCR CLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a93859-f0ad-48d2-b5ff-83bef88f6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!onnxsim cls.onnx cls_fixed.onnx --overwrite-input-shape \"x:1,3,48,192\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4f0b9-204e-4b71-aeac-8cf342737dad",
   "metadata": {},
   "source": [
    "### 5.4. Fix input shape of TEXT Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7053d-b9e5-41ea-8d93-2eb3d3767710",
   "metadata": {},
   "source": [
    "Since the NPU requires fixed input shapes, we use six separate models with different aspect ratios to improve recognition accuracy.\n",
    "\n",
    "For each case, we select and apply the model that best matches the ratio of the detected text.\n",
    "\n",
    "<img src=\"assets/ocr-ratio.png\" style=\"max-width: 1000px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc078c-b815-495f-a124-210f5ea5bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!onnxsim rec.onnx rec_fixed_ratio_3.onnx --overwrite-input-shape \"x:1,3,48,120\"\n",
    "!onnxsim rec.onnx rec_fixed_ratio_5.onnx --overwrite-input-shape \"x:1,3,48,240\"\n",
    "!onnxsim rec.onnx rec_fixed_ratio_10.onnx --overwrite-input-shape \"x:1,3,48,480\"\n",
    "!onnxsim rec.onnx rec_fixed_ratio_15.onnx --overwrite-input-shape \"x:1,3,48,720\"\n",
    "!onnxsim rec.onnx rec_fixed_ratio_25.onnx --overwrite-input-shape \"x:1,3,48,1200\"\n",
    "!onnxsim rec.onnx rec_fixed_ratio_35.onnx --overwrite-input-shape \"x:1,3,48,1920\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05703e-8e04-4ba4-8721-75b86e654bf9",
   "metadata": {},
   "source": [
    "### 5.5. Verify all fixed input ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf22916-21f5-4449-a437-d76bd7d5093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "def print_input_shape(model_path):\n",
    "    session = ort.InferenceSession(model_path)\n",
    "    print(f\"---- Input Shapes of {model_path} ----\")\n",
    "    for input_meta in session.get_inputs():\n",
    "        print(f\"Name: {input_meta.name}, Shape: {input_meta.shape}, Type: {input_meta.type}\")\n",
    "        print(\" \" * 30)\n",
    "\n",
    "print_input_shape(\"det_fixed.onnx\")\n",
    "print_input_shape(\"cls_fixed.onnx\")\n",
    "print_input_shape(\"rec_fixed_ratio_3.onnx\")\n",
    "print_input_shape(\"rec_fixed_ratio_5.onnx\")\n",
    "print_input_shape(\"rec_fixed_ratio_10.onnx\")\n",
    "print_input_shape(\"rec_fixed_ratio_15.onnx\")\n",
    "print_input_shape(\"rec_fixed_ratio_25.onnx\")\n",
    "print_input_shape(\"rec_fixed_ratio_35.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081aca4d-08c3-45a5-98f2-18cb7de7b3ce",
   "metadata": {},
   "source": [
    "## 6. Compile to *.dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee48843-12f2-42a7-840a-b19187cf703c",
   "metadata": {},
   "source": [
    "### 6.1. Compile TEXT Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f100c1-ecfa-47f3-b910-92ab2747110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile det.json\n",
    "{\n",
    "  \"inputs\": { \"x\": [1, 3, 480, 480] },\n",
    "  \"calibration_num\": 100,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"det_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      { \"resize\": { \"width\": 480, \"height\": 480 } },\n",
    "      { \"convertColor\": { \"form\": \"BGR2RGB\" } },\n",
    "      { \"div\": { \"x\": 255 } },\n",
    "      { \"normalize\": { \"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225] } },\n",
    "      { \"transpose\": { \"axis\": [2, 0, 1] } },\n",
    "      { \"expandDim\": { \"axis\": 0 } }\n",
    "    ]\n",
    "  },\n",
    "  \"enhanced_scheme\": { \"DXQ-P0\": { \"alpha\": 0.5 } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b56d6-6eff-46b2-89a6-7ea50738b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$root_path/dx-all-suite/dx-compiler/dx_com/dx_com/dx_com -m det_fixed.onnx -c det.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1590617-dbd9-4ba3-a70d-e5ff8a4cfabe",
   "metadata": {},
   "source": [
    "### 6.2. Compile Orientation Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320b96a-cd3e-4ac1-b7e4-59203441e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cls.json\n",
    "{\n",
    "  \"inputs\": { \"x\": [1, 3, 48, 192] },\n",
    "  \"calibration_num\": 80,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"rec_dataset/ratio_5\",\n",
    "    \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      { \"resize\": { \"width\": 192, \"height\": 48 } },\n",
    "      { \"convertColor\": { \"form\": \"BGR2RGB\" } },\n",
    "      { \"div\": { \"x\": 255 } },\n",
    "      { \"normalize\": { \"mean\": [0.5, 0.5, 0.5], \"std\": [0.5, 0.5, 0.5] } },\n",
    "      { \"transpose\": { \"axis\": [2, 0, 1] } }\n",
    "    ]\n",
    "  },\n",
    "  \"enhanced_scheme\": { \"DXQ-P0\": { \"alpha\": 0.5 } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d60a3-7a12-4ba1-89ae-7339312daad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$root_path/dx-all-suite/dx-compiler/dx_com/dx_com/dx_com -m cls_fixed.onnx -c cls.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26e1b1-96dc-4813-b1df-c73295f437b4",
   "metadata": {},
   "source": [
    "### 6.3. Compile TEXT Recognition Models (Ratio x2.5 / W:48 / H:120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4c776-a2e2-4fb2-8043-f9d8ee88f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rec_ratio_3.json\n",
    "{\n",
    "  \"inputs\": { \"x\": [1, 3, 48, 120] },\n",
    "  \"calibration_num\": 80,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"rec_dataset/ratio_5\",\n",
    "    \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      { \"resize\": { \"width\": 120, \"height\": 48 } },\n",
    "      { \"convertColor\": { \"form\": \"BGR2RGB\" } },\n",
    "      { \"div\": { \"x\": 255 } },\n",
    "      { \"normalize\": { \"mean\": [0.5, 0.5, 0.5], \"std\": [0.5, 0.5, 0.5] } },\n",
    "      { \"transpose\": { \"axis\": [2, 0, 1] } }\n",
    "    ]\n",
    "  },\n",
    "  \"enhanced_scheme\": { \"DXQ-P0\": { \"alpha\": 0.5 } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a3fed-8928-4e06-8523-668a7bc877d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$root_path/dx-all-suite/dx-compiler/dx_com/dx_com/dx_com -m rec_fixed_ratio_3.onnx -c rec_ratio_3.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb6191-c44d-4580-8920-5bb23841696c",
   "metadata": {},
   "source": [
    "### 6.4. Compile TEXT Recognition Models (Ratio x5 / W:48 / H:240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3c23e-9d9f-40ff-ae1a-d885618e476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rec_ratio_5.json\n",
    "{\n",
    "  \"inputs\": { \"x\": [1, 3, 48, 240] },\n",
    "  \"calibration_num\": 80,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"rec_dataset/ratio_5\",\n",
    "    \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      { \"resize\": { \"width\": 240, \"height\": 48 } },\n",
    "      { \"convertColor\": { \"form\": \"BGR2RGB\" } },\n",
    "      { \"div\": { \"x\": 255 } },\n",
    "      { \"normalize\": { \"mean\": [0.5, 0.5, 0.5], \"std\": [0.5, 0.5, 0.5] } },\n",
    "      { \"transpose\": { \"axis\": [2, 0, 1] } }\n",
    "    ]\n",
    "  },\n",
    "  \"enhanced_scheme\": { \"DXQ-P0\": { \"alpha\": 0.5 } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36f38d-4b6d-4e48-adbd-0140ea68417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$root_path/dx-all-suite/dx-compiler/dx_com/dx_com/dx_com -m rec_fixed_ratio_5.onnx -c rec_ratio_5.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb56a2-c1ae-4783-aed1-10b3d2b1a6ea",
   "metadata": {},
   "source": [
    "### 6.5. Compile TEXT Recognition Models (Ratio x10 / W:48 / H:480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792501a-c77f-4eee-b3e1-9c53ed565603",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rec_ratio_10.json\n",
    "{\n",
    "  \"inputs\": { \"x\": [1, 3, 48, 480] },\n",
    "  \"calibration_num\": 80,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"rec_dataset/ratio_15\",\n",
    "    \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      { \"resize\": { \"width\": 480, \"height\": 48 } },\n",
    "      { \"convertColor\": { \"form\": \"BGR2RGB\" } },\n",
    "      { \"div\": { \"x\": 255 } },\n",
    "      { \"normalize\": { \"mean\": [0.5, 0.5, 0.5], \"std\": [0.5, 0.5, 0.5] } },\n",
    "      { \"transpose\": { \"axis\": [2, 0, 1] } }\n",
    "    ]\n",
    "  },\n",
    "  \"enhanced_scheme\": { \"DXQ-P0\": { \"alpha\": 0.5 } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0a979-9a2b-48a1-a3c1-f75134ddaebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$root_path/dx-all-suite/dx-compiler/dx_com/dx_com/dx_com -m rec_fixed_ratio_10.onnx -c rec_ratio_10.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602c8c2-1c53-460c-b04b-88fab8afcc49",
   "metadata": {},
   "source": [
    "### 6.6. Compile TEXT Recognition Models (Ratio x15 / W:48 / H:720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23670275-4ba7-4a74-9438-167c5f043b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rec_ratio_15.json\n",
    "{\n",
    "  \"inputs\": { \"x\": [1, 3, 48, 720] },\n",
    "  \"calibration_num\": 80,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"rec_dataset/ratio_15\",\n",
    "    \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      { \"resize\": { \"width\": 720, \"height\": 48 } },\n",
    "      { \"convertColor\": { \"form\": \"BGR2RGB\" } },\n",
    "      { \"div\": { \"x\": 255 } },\n",
    "      { \"normalize\": { \"mean\": [0.5, 0.5, 0.5], \"std\": [0.5, 0.5, 0.5] } },\n",
    "      { \"transpose\": { \"axis\": [2, 0, 1] } }\n",
    "    ]\n",
    "  },\n",
    "  \"enhanced_scheme\": { \"DXQ-P0\": { \"alpha\": 0.5 } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b2dad-a157-4728-a4bf-52a087b4e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$root_path/dx-all-suite/dx-compiler/dx_com/dx_com/dx_com -m rec_fixed_ratio_15.onnx -c rec_ratio_15.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc6c91-f75d-4c33-9719-0888a05ee4f8",
   "metadata": {},
   "source": [
    "### 6.7. Compile TEXT Recognition Models (Ratio x25 / W:48 / H:1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c29971-af40-4921-a3f1-4efddf7a090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rec_ratio_25.json\n",
    "{\n",
    "  \"inputs\": { \"x\": [1, 3, 48, 1200] },\n",
    "  \"calibration_num\": 80,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"rec_dataset/ratio_25\",\n",
    "    \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      { \"resize\": { \"width\": 1200, \"height\": 48 } },\n",
    "      { \"convertColor\": { \"form\": \"BGR2RGB\" } },\n",
    "      { \"div\": { \"x\": 255 } },\n",
    "      { \"normalize\": { \"mean\": [0.5, 0.5, 0.5], \"std\": [0.5, 0.5, 0.5] } },\n",
    "      { \"transpose\": { \"axis\": [2, 0, 1] } }\n",
    "    ]\n",
    "  },\n",
    "  \"enhanced_scheme\": { \"DXQ-P0\": { \"alpha\": 0.5 } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27fa01a-393c-4540-a775-d98178c92e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$root_path/dx-all-suite/dx-compiler/dx_com/dx_com/dx_com -m rec_fixed_ratio_25.onnx -c rec_ratio_25.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c2074-5d66-4bee-820b-7be49a868cc3",
   "metadata": {},
   "source": [
    "### 6.8. Compile TEXT Recognition Models (Ratio x35 / W:48 / H:1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97906ddd-a0b0-42be-84d2-4280570871f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rec_ratio_35.json\n",
    "{\n",
    "  \"inputs\": { \"x\": [1, 3, 48, 1920] },\n",
    "  \"calibration_num\": 80,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"rec_dataset/ratio_25\",\n",
    "    \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      { \"resize\": { \"width\": 1920, \"height\": 48 } },\n",
    "      { \"convertColor\": { \"form\": \"BGR2RGB\" } },\n",
    "      { \"div\": { \"x\": 255 } },\n",
    "      { \"normalize\": { \"mean\": [0.5, 0.5, 0.5], \"std\": [0.5, 0.5, 0.5] } },\n",
    "      { \"transpose\": { \"axis\": [2, 0, 1] } }\n",
    "    ]\n",
    "  },\n",
    "  \"enhanced_scheme\": { \"DXQ-P0\": { \"alpha\": 0.5 } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1261ae-b96b-413a-8779-630be596f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$root_path/dx-all-suite/dx-compiler/dx_com/dx_com/dx_com -m rec_fixed_ratio_35.onnx -c rec_ratio_35.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c7127-cf53-42e0-b160-b645c1106694",
   "metadata": {},
   "source": [
    "Verify if all required *.dxnn files are compiled:\n",
    "\n",
    "* **Expected results**:\n",
    "   ```\n",
    "   cls_fixed.dxnn\t\t rec_fixed_ratio_15.dxnn  rec_fixed_ratio_3.dxnn\n",
    "   det_fixed.dxnn\t\t rec_fixed_ratio_25.dxnn  rec_fixed_ratio_5.dxnn\n",
    "   rec_fixed_ratio_10.dxnn  rec_fixed_ratio_35.dxnn\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d43cc-c0bb-4344-97f2-e88c5d5769f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d480886-c3df-48f6-870b-47625918d995",
   "metadata": {},
   "source": [
    "## 7. Implement OCR Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92f519-ad3c-4957-952c-9cd835d2d9aa",
   "metadata": {},
   "source": [
    "To handle text images with different shapes (dynamic ratios), we couldn't use a single fixed input size. So, we split the ratio range into segments and used a different input shape.\n",
    "\n",
    "Following the AI pipeline illustrated, this AI pipeline is one of OCR concepts based on NPU.\n",
    "\n",
    "<img src=\"assets/ocr-npu-pipeline.jpg\" style=\"max-width: 1000px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac4c6d-e70b-4676-8610-d1301346ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to T10-PaddleOCR path\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/notebooks/T10-PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5efe63-f623-4a22-8992-44ee8705f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82b1dd-a43a-44e3-bedb-1697f03d8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $root_path/dx-all-suite/dx-runtime/dx_rt/python_package && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a60874-7c94-49cd-99e5-67b9bd6d107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can stop the app by 'q' keyboard input\n",
    "!python3 main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ae470-5a4d-4b57-9e70-db151ff80fa1",
   "metadata": {},
   "source": [
    "<img src=\"assets/paddleocr-result.png\" style=\"max-width: 1000px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a844fa-506c-48e7-bd67-f2aacd149d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
