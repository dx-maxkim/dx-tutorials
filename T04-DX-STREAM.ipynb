{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88efc8e-edb8-4f01-bcdf-dffc19f2533a",
   "metadata": {},
   "source": [
    "# DEEPX Tutorial 04 - DX STREAM workflow\n",
    "\n",
    "This forth tutorial intruduces DX-STREAM overview and end-to-end workflow for a custom AI model with DX-STREAM.\n",
    "\n",
    " - Understand what DX-STREAM is and where it fits in the DEEPX SDK\n",
    " - Learn the core elements (DxPreprocess â†’ DxInfer â†’ DxPostprocess â†’ DxTracker â†’ DxOsd)\n",
    " - Implement custom preprocess/postprocess libraries to support customized AI models\n",
    "\n",
    ">This tutorial is based on dx-all-suite v2.0.0, released in September 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b657a27-5033-462a-a1fd-5e3c2dc41038",
   "metadata": {},
   "source": [
    "## 0. DX-STREAM Overview\n",
    "A set of GStreamer elements for Vision AI on DEEPX NPUs.\n",
    "```text\n",
    "Source â†’ (decodebin) â†’ DxPreprocess â†’ DxInfer â†’ DxPostprocess â†’ DxTracker â†’ DxOsd â†’ Sink\n",
    "```\n",
    "- **DxPreprocess**: resize/ROI/color space, optional custom library\n",
    "- **DxInfer**: (Inference) runs .dxnn model, links to preprocess via `preprocess-id`\n",
    "- **DxPostprocess**: decode tensors â†’ metadata, optional custom library\n",
    "- **DxTracker**: assign stable IDs (e.g., OC_SORT)\n",
    "- **DxOsd**: (On-Screen Display) draw boxes/labels/poses/segmentation onto frames\n",
    "\n",
    "  ![img](assets/dx-stream-pipeline.png)\n",
    "\n",
    "For more details, download DX_STREAM User Guide from ðŸ‘‰ [here](https://developer.deepx.ai/?files=MjUxNw==)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfffb69-ea1d-4b7b-bc95-4bc2881a6493",
   "metadata": {},
   "source": [
    "## 1. Prerequisites:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0036c6-b9d5-4efd-90c9-be411294f7d4",
   "metadata": {},
   "source": [
    "This section reconfirms whether DX-STREAM has been properly installed. It assumes that the installation was completed in Tutorial 01 (T01).\n",
    "\n",
    "Move to `dx_stream` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f641d-f931-4363-ba8e-0e75927620a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-runtime/dx_stream\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-runtime/dx_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3026b6-839f-4cb5-8312-91f2a829ae42",
   "metadata": {},
   "source": [
    "Download required models and sample videos by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15b670-4f56-4375-8f65-b39e5d59fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assets (models + videos) are downloaded and placed in the assets/ directory.\n",
    "!./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b9fdd-52ce-461e-ba79-afa9ab120419",
   "metadata": {},
   "source": [
    "3. Verify required plugins - following dx-plugins should be displayed:\n",
    "  - dxgather: DxGather\n",
    "  - dxinfer: DXInfer\n",
    "  - dxinputselector: DXInputSelector\n",
    "  - dxmsgbroker: DXMsgBroker\n",
    "  - dxmsgconv: DXMsgConv\n",
    "  - dxosd: DXOsd\n",
    "  - dxoutputselector: DXOutputSelector\n",
    "  - dxpostprocess: DXPostprocess\n",
    "  - dxpreprocess: DXPreprocess\n",
    "  - dxrate: DXRate\n",
    "  - dxtracker: DXTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe16065-10b4-48a3-8cad-f59a063e2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if dx_stream related plugins are installed as expected with 'gst-inspect-1.0'\n",
    "!gst-inspect-1.0 dxstream || echo 'dxstream plugin not found'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c9d96-890c-4e8b-af0b-dcf0332d324a",
   "metadata": {},
   "source": [
    "## 2. Quick Starts with run_demo scripts in DX_STREAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c525661-2ffc-49f9-96c0-6726962802eb",
   "metadata": {},
   "source": [
    "### 2.1. Let's run the YOLOv7 pipeline with video input:\n",
    "   - `Note`: You can stop the pipeline by clicking the stop button ('â– ') above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306a3e6-b27b-47d1-acfd-615894f8b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video file as an input\n",
    "VIDEO_SRC='dx_stream/samples/videos/doughnut.mp4'\n",
    "\n",
    "# GStreamer pipeline configuration\n",
    "!gst-launch-1.0 filesrc location=$VIDEO_SRC ! decodebin ! \\\n",
    "                   dxpreprocess config-file-path=dx_stream/configs/Object_Detection/YoloV7/preprocess_config.json ! queue ! \\\n",
    "                   dxinfer config-file-path=dx_stream/configs/Object_Detection/YoloV7/inference_config.json ! queue ! \\\n",
    "                   dxpostprocess config-file-path=dx_stream/configs/Object_Detection/YoloV7/postprocess_config.json ! queue ! \\\n",
    "                   dxosd width=1280 height=720 ! queue ! \\\n",
    "                   videoconvert ! fpsdisplaysink sync=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c3521c-bc02-4423-8010-ca73dbdbb367",
   "metadata": {},
   "source": [
    "### 2.2. Let's learn about the definition and usage of each plugin using **gst-inspect-1.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f6a38-0449-4cd1-b01f-64c068d5af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dxpreprocess plugin (DXPreprocess â€“ Performs input preprocessing for AI inference)\n",
    "!gst-inspect-1.0 dxpreprocess\n",
    "!echo \"==============[ Configuration Start ]==============\"\n",
    "!cat dx_stream/configs/Object_Detection/YoloV7/preprocess_config.json\n",
    "!echo \"==============[  Configuration End  ]==============\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842df0d0-e7de-45fb-8ba7-4f5006bc0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dxinfer plugin (DXInfer â€“ Executes AI model inference using the DEEPX NPU)\n",
    "!gst-inspect-1.0 dxinfer\n",
    "!echo \"==============[ Configuration Start ]==============\"\n",
    "!cat dx_stream/configs/Object_Detection/YoloV7/inference_config.json\n",
    "!echo \"==============[  Configuration End  ]==============\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5928ed-ed35-4d3e-a07d-cefe7e866a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dxpostprocess plugin (DXPostprocess â€“ Processes and formats inference output)\n",
    "!gst-inspect-1.0 dxpostprocess\n",
    "!echo \"==============[ Configuration Start ]==============\"\n",
    "!cat dx_stream/configs/Object_Detection/YoloV7/postprocess_config.json\n",
    "!echo \"==============[  Configuration End  ]==============\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3ddf6-7c05-4f82-822b-84b4bd755494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dxosd plugin (Overlays inference results onto video frames)\n",
    "!gst-inspect-1.0 dxosd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac29565-bf58-49b1-af77-7e971086f498",
   "metadata": {},
   "source": [
    "## 3. Run Demo Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be5fb3-7c6a-47ae-aade-a4b5eed78acc",
   "metadata": {},
   "source": [
    "There is a **`run_demo.sh`** script file in the dx_stream path, which contains main examples of using dx_stream.\n",
    "\n",
    "This script calls the following script based on the user's selection (from 0 to 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565573f0-eb7e-4d0e-a2d0-f14b1fd415a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the demo list - Total 8 GStreamer demos\n",
    "!tail -n 15 run_demo.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ff311-35d2-4644-9e18-fa2b3e3fea1c",
   "metadata": {},
   "source": [
    "Let's run each demo by changing the argument from '0' to '7':\n",
    "   - `Note`: You can stop the pipeline by clicking the stop button ('â– ') above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea4b09-a02a-4604-b015-f21031893723",
   "metadata": {},
   "source": [
    "### 3.1. Single_network/object_detection - YOLOv7\n",
    "![img](assets/pipline-single-detection.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96b0aa-f55a-4ee6-8403-4d50a16ecead",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_demo.sh <<< 0 # single_network/object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776d750-f8c1-4309-b27b-ff5769563917",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dx_stream/pipelines/single_network/object_detection/run_YOLOV7.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b2d6b-a3d2-40ab-aa8a-7935056a4ebe",
   "metadata": {},
   "source": [
    "### 3.2. Single_network/object_detection - YOLOv5 Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11514b9b-ae99-4d23-9332-81fe13a824aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_demo.sh <<< 1 # single_network/face_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5aaf18-0155-419e-a739-efe1d2741a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dx_stream/pipelines/single_network/face_detection/run_YOLOFACE.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c954d-c387-4686-861b-d1c5cef33d69",
   "metadata": {},
   "source": [
    "Let's try camera input instead of video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230219d-19df-42e5-8e45-0cc8bc022a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam input with RAW image format\n",
    "!gst-launch-1.0 \\\n",
    "  v4l2src device=/dev/video0 do-timestamp=true ! \\\n",
    "  videoconvert ! video/x-raw,width=1920,height=1080 ! queue ! \\\n",
    "  dxpreprocess config-file-path=dx_stream/configs/Face_Detection/YOLOV5S_Face/preprocess_config.json ! queue ! \\\n",
    "  dxinfer config-file-path=dx_stream/configs/Face_Detection/YOLOV5S_Face/inference_config.json  ! queue ! \\\n",
    "  dxpostprocess config-file-path=dx_stream/configs/Face_Detection/YOLOV5S_Face/postprocess_config.json ! queue ! \\\n",
    "  dxosd width=1920 height=1080 ! queue ! \\\n",
    "  videoconvert ! fpsdisplaysink sync=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67062fa8-b256-49a1-9ca9-3193bfb7e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam input with MJPEG format (some USB webcam does NOT support MJPEG format)\n",
    "!gst-launch-1.0 \\\n",
    "  v4l2src device=/dev/video0 do-timestamp=true ! \\\n",
    "  image/jpeg,width=1920,height=1080,framerate=30/1 ! jpegdec ! videoconvert ! queue ! \\\n",
    "  dxpreprocess config-file-path=dx_stream/configs/Face_Detection/YOLOV5S_Face/preprocess_config.json ! queue ! \\\n",
    "  dxinfer config-file-path=dx_stream/configs/Face_Detection/YOLOV5S_Face/inference_config.json  ! queue ! \\\n",
    "  dxpostprocess config-file-path=dx_stream/configs/Face_Detection/YOLOV5S_Face/postprocess_config.json ! queue ! \\\n",
    "  dxosd width=1920 height=1080 ! queue ! \\\n",
    "  videoconvert ! fpsdisplaysink sync=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075896c6-1fa4-4f0e-a462-bd8f66eb1cb7",
   "metadata": {},
   "source": [
    "### 3.3. Single_network/object_detection - Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4eee5-d92a-437e-8fe0-c84cb255b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_demo.sh <<< 3 # single_network/pose_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381231b1-89b9-467c-8b9c-5ceb07f6b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dx_stream/pipelines/single_network/pose_estimation/run_YOLOPOSE.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e881792-4c17-4b20-9f70-6bb78a971ba8",
   "metadata": {},
   "source": [
    "### 3.4. Single_network/object_detection - Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11ec9b-08c1-4cf7-9005-7d478b9eb0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_demo.sh <<< 4 # single_network/semantic_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc530d-b645-4f91-8fcb-79461215da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dx_stream/pipelines/single_network/semantic_segmentation/run_DeepLabV3PlusMobileNetV2.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbecfb-314b-413a-a678-df2d4dba8a54",
   "metadata": {},
   "source": [
    "### 3.5. Tracker - YOLOV5S tracker\n",
    "![img](assets/pipline-single-tracking.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d060e-01f7-4299-806f-1c1d23f7a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_demo.sh <<< 2 # tracking/run_YOLOV5S_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ebc2c-2141-4280-81b0-62b49d0c200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dx_stream/pipelines/tracking/run_YOLOV5S_tracker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a215d-febf-4e10-8799-3f50487645e0",
   "metadata": {},
   "source": [
    "### 3.6. Multi-Stream - YOLOv5S\n",
    "![img](assets/pipeline-multi-stream.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd097649-57dd-48e4-8253-63eb3a9c481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_demo.sh <<< 5 # multi_stream/run_multi_stream_YOLOV5S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e1103-39d1-4b03-ad46-86bf06406586",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dx_stream/pipelines/multi_stream/run_multi_stream_YOLOV5S.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670f4b1-f15e-4a9a-935c-504a08b4ee09",
   "metadata": {},
   "source": [
    "<img src=\"assets/pipeline-multi-stream-single-infer.png\" style=\"max-width: 1400px;\">\n",
    "\n",
    "When receiving multi-channel inputs but using a single AI model, there's no need to deploy the dxinfer plugin for each input, as shown above. Sharing a single inference pipeline saves DRAM space within the NPU and allows for more efficient model operation.\n",
    "\n",
    "The DX-M1 has 4GB of DRAM. If the model size is over 1.1GB, deploying four dxinfer plugins as shown above will exceed the M1's DRAM capacity and render the model unusable. Therefore, in this case, a multi-stream and single-inference pipeline design is highly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da634f5-5630-4dff-884f-a7efcb91d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dx_stream/pipelines/multi_stream/run_multi_stream_single_infer_YOLOV5S.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7194f-efa9-4f77-a3e0-1ba5fa21d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dx_stream/pipelines/multi_stream/run_multi_stream_single_infer_YOLOV5S.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec661340-e3b3-4f03-a5f1-98cc8e73b2ba",
   "metadata": {},
   "source": [
    "### 3.7. Multi-Stream - RTSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b98cb-cca6-456f-a374-558c5d59917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_demo.sh <<< 6 # rtsp/run_RTSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24009ef0-a269-4ea1-b94e-42137374dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dx_stream/pipelines/rtsp/run_RTSP.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9a875-da1b-4fd5-994b-d186481f1420",
   "metadata": {},
   "source": [
    "### 3.8. Secondary Mode\n",
    "![img](assets/pipeline-secondary.png)\n",
    "- **Primary Mode** applies preprocessing to the entire frame. If object detection is performed within the\n",
    "same pipeline, DxPreprocess will operate in this mode.\n",
    "- **Secondary Mode** applies preprocessing to individual object regions detected in this frame. This mode\n",
    "requires object metadata (e.g., from an upstream object detection element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7ff49-89f2-497f-a96a-db12cb76e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_demo.sh <<< 7 # secondary_mode/run_secondary_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8bb1d4-1b9a-4467-bf7b-54971ad9439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dx_stream/pipelines/secondary_mode/run_secondary_mode.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db699b59-2692-49a1-b115-59a8e8120630",
   "metadata": {},
   "source": [
    "## 4. Writeing Your Own Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472086d-3ecd-4ed8-adb0-33b58a7dc7d8",
   "metadata": {},
   "source": [
    "![img](assets/custom-pipeline.png)\n",
    "\n",
    "This chapter describes how to integrate a custom AI model and implement user-defined logic within the\n",
    "DX-STREAM pipeline.\n",
    "\n",
    "This guide focuses on how to configure and integrate custom logic into the DX-STREAM pipeline using\n",
    "modular elements such as DxPreprocess, DxInfer, and DxPostprocess.\n",
    "\n",
    "We will reuse the custom model (Forklift & Worker detector) in the previous tutorial-03. Therefore, need to customize the dxpostporcess plugin.\n",
    "\n",
    "<img src=\"assets/detection-goal.jpg\" style=\"max-width: 1400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc17e78-b854-40ea-b591-ab907be1ae69",
   "metadata": {},
   "source": [
    "### 4.1. Download complied DXNN file from the following link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ad22c-dba7-4426-bf4d-839629092c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"cs.deepx.ai/_deepx_fae_archive/dx-tutorials/yolov7-forklift-person.dxnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e09e31-46e1-4c08-821f-60b67fbe4479",
   "metadata": {},
   "source": [
    "### 4.2. Modify \"dx_stream/custom_library/postprocess_library/YoloV7/postprocess.cpp\" file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c283cd01-b102-4993-93d2-66b8b78dda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_TEXT = r\"\"\"diff --git a/dx_stream/custom_library/postprocess_library/YoloV7/postprocess.cpp b/dx_stream/custom_library/postprocess_library/YoloV7/postprocess.cpp\n",
    "index 7d048e9..26125ca 100644\n",
    "--- a/dx_stream/custom_library/postprocess_library/YoloV7/postprocess.cpp\n",
    "+++ b/dx_stream/custom_library/postprocess_library/YoloV7/postprocess.cpp\n",
    "@@ -58,21 +58,11 @@ struct YoloConfig {\n",
    "     float nms_threshold = 0.4f;      // IoU threshold for NMS\n",
    "\n",
    "     // Number of classes in your dataset\n",
    "-    int num_classes = 80;\n",
    "+    int num_classes = 2;\n",
    "\n",
    "     // COCO dataset class names (modify for your dataset)\n",
    "     std::vector<std::string> class_names = {\n",
    "-        \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\",\n",
    "-        \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "-        \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\",\n",
    "-        \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "-        \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
    "-        \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "-        \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\",\n",
    "-        \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
    "-        \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\",\n",
    "-        \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "-        \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "+        \"Forklift\", \"Worker\"\n",
    "     };\n",
    " };\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# save as a file\n",
    "with open(\"dx_stream_update.diff\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(DIFF_TEXT)\n",
    "\n",
    "print(\"Saved: dx_stream_update.diff (bytes)\", len(DIFF_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99300027-6ba4-4730-bc26-a9ef6ceeaaca",
   "metadata": {},
   "source": [
    "### 4.3. Apply the modified code by using git apply command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a64b2-7220-4b34-8336-c405fec3db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git checkout -- dx_stream/custom_library/postprocess_library/YoloV7/postprocess.cpp\n",
    "!git apply --whitespace=fix dx_stream_update.diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03033c18-29b6-4da9-ba15-9fb95e0307ba",
   "metadata": {},
   "source": [
    "### 4.4. Re-build DX_STREAM libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a5a13-d73d-47e3-be6b-695f8a021931",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ../venv-dx-runtime/bin/activate && ./build.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ae0eb-c0d9-475e-ac57-3e48a5fbc025",
   "metadata": {},
   "source": [
    "### 4.5. Create configuration json to load your custom model (yolov7-forklift-person.dxnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d834565-155b-4efd-8083-566f095298a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "yolov7_custom = {\n",
    "    \"preprocess_id\": 1,\n",
    "    \"inference_id\": 1,\n",
    "    \"model_path\" : \"./yolov7-forklift-person.dxnn\"\n",
    "}\n",
    "with open(\"yolov7-forklift-person.json\", \"w\") as f: json.dump(yolov7_custom, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e347ec-1ce8-4174-a6b8-d8f959fa4b3c",
   "metadata": {},
   "source": [
    "### 4.6. Download a test video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a147f-85a7-48f4-8d21-a86ab003efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"cs.deepx.ai/_deepx_fae_archive/dx-tutorials/forklift-worker.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb2736-423b-4e1e-8ca9-27f9db2653c5",
   "metadata": {},
   "source": [
    "### 4.7. Run the following DX-STREAM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669200a-7ac0-4a13-b5b4-cebdd5d16823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video file as an input\n",
    "VIDEO_SRC='forklift-worker.mp4'\n",
    "\n",
    "# GStreamer pipeline configuration\n",
    "!gst-launch-1.0 filesrc location=$VIDEO_SRC ! decodebin ! \\\n",
    "                   dxpreprocess config-file-path=dx_stream/configs/Object_Detection/YoloV7/preprocess_config.json ! queue ! \\\n",
    "                   dxinfer config-file-path=yolov7-forklift-person.json ! queue ! \\\n",
    "                   dxpostprocess config-file-path=dx_stream/configs/Object_Detection/YoloV7/postprocess_config.json ! queue ! \\\n",
    "                   dxosd width=1280 height=720 ! queue ! \\\n",
    "                   videoconvert ! fpsdisplaysink sync=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8106bd3d-2890-4be7-a118-8865fd905772",
   "metadata": {},
   "source": [
    "## 5. Debug Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41369033-aaf3-404a-bb25-c467174d7dad",
   "metadata": {},
   "source": [
    "Dump GSTreamer pipiline configurations as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef4240-1267-47d7-ba90-bd70f61d830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install -y graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57466903-11dc-4cac-96d1-1c526e1d46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.dot\n",
    "!export GST_DEBUG_DUMP_DOT_DIR=./ && ./run_demo.sh <<< 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c42f2-fcf7-49a1-891c-d9bf2523f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a374ef-0e53-4593-b6b5-5a17467e1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!dot -Tpng -o NULL_READY.png *NULL_READY.dot\n",
    "#!dot -Tpng -o READY_PAUSED.png *READY_PAUSED.dot\n",
    "!dot -Tpng -o PAUSED_PLAYING.png *PAUSED_PLAYING.dot\n",
    "#!dot -Tpng -o PLAYING_PAUSED.png *PLAYING_PAUSED.dot\n",
    "# -Tpng : output format is 'png'\n",
    "# -o : output file name\n",
    "# *.dot : input file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f5fe9-508c-4168-9df5-f1749aeeb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Show the image\n",
    "display(Image(filename=\"PAUSED_PLAYING.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
