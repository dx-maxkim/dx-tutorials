{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71513b35-9e60-4c49-b3ac-45c66d7f3933",
   "metadata": {},
   "source": [
    "# DEEPX Tutorial 05 - DX Compile workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38306cf-27f3-454b-9794-eb4ce4ee76a3",
   "metadata": {},
   "source": [
    "In Tutorial 5, you will practice compiling classification, object detection, and segmentation models using the DX-Compiler. We will also review the guide for troubleshooting problems during compilation.\n",
    "\n",
    "For more details, refer to the DX-COM user guide ðŸ‘‰ [Download](https://developer.deepx.ai/?files=MjUxNQ==)\n",
    "\n",
    ">This tutorial is based on dx-all-suite v2.0.0, released in September 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3d09f-4ccd-4589-8c7a-aa58cfd510fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Compiling Image Classification Model (MobileNetV2)\n",
    "1. Export PyTorch â†’ ONNX\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cbee8-6cbc-4a78-8700-1cdc65076c4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Export PyTorch â†’ ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b5de9-ab8a-466e-bfba-823f0fe96817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch & onnx\n",
    "!pip install --quiet torch torchvision onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87986a8c-af2b-43f5-8860-c63f0bc14ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639e9cb-7a79-422a-93e4-700ad8fcf472",
   "metadata": {},
   "source": [
    "Export Pytorch based MobileNetV2 model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e609d-ba12-45e5-8f40-f0d443165d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "# Load MobileNetV2 model\n",
    "model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Batch size must be 1\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "onnx_path = \"MobilenetV2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                      # PyTorch model object to export\n",
    "    dummy_input,                # Dummy input used for tracing (tuple is possible)\n",
    "    onnx_path,                  # Output ONNX file path\n",
    "    export_params=True,         # If True, saves model parameter (weight) into the ONNX file\n",
    "    input_names=[\"input_test\"], # Name of the ONNX model input tensor\n",
    "    output_names=[\"output\"],    # Name of the ONNX model output tensor\n",
    "    opset_version=13            # ONNX opset version (recommended: 11 ~ 21)\n",
    ")\n",
    "print(\"âœ… Save ONNX:\", onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435e445-e87e-4706-ad8f-09d040477cb0",
   "metadata": {},
   "source": [
    "### 2. JSON configuration for Input/Pre-processing/Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e037ae0-6ebd-4429-a5a6-70d56c9f818c",
   "metadata": {},
   "source": [
    "Generate a configuration file for Input/Pre-processing/Calibration of MobilenetV2.\n",
    "\n",
    "This JSON configuration file incudes:\n",
    " - Input specifications\n",
    " - Calibration methods\n",
    " - Data preprocessing settings\n",
    " - Optional parameters for advanced compilation schemes\n",
    "\n",
    "Model **Input Restrictions**:\n",
    " - The batch size must be fixed to 1\n",
    " - Only a single input is supported (Multi-input will be supported in 2026)\n",
    " - Input name must exactly match ONNX model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e10ab-7804-4c31-b4d0-e39dd16749d9",
   "metadata": {},
   "source": [
    "#### 2.1. Incorrect input name case\n",
    "\n",
    "This MobilenetV2 model's input name is `input_test` as shown below:\n",
    "\n",
    "![](assets/mobilenetv2-input-name.png)\n",
    "\n",
    "Input name (`input_test` in this example) must exactly match the input name of JSON configuration.\n",
    "\n",
    "However, you can see there is wrong input name `incorrect_input_name` in the following JSON configuration for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbe57234-a230-4925-be56-b3a410f26ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MobilenetV2.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"incorrect_input_name\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47876a-1379-4bce-be6c-6889a0cb1397",
   "metadata": {},
   "source": [
    "Let's run DX-COM with this wrong JSON configuration. You will meet ERROR:\n",
    "> ConfigInputError: The input name in config incorrect_input_name is not same as model input input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a8e81-84d6-4e3d-93ba-e7026b4282cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01180d0a-2bfc-4ef8-8ed0-be93cbc8f755",
   "metadata": {},
   "source": [
    "#### 2.2. Incorrect input shape\n",
    "In the following JSON file, the input name has the correct one - `input_test`.\n",
    "\n",
    "However, even if your input shape is 1x3x224x224 (BxCxHxW), calibration image shape has 224x224x3 (HxWxC). You must change the shape of calibraiton image to match with 1x3x224x224 (BxCxHxW) by using `transpose` and `expandDim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0655c076-5ac0-4049-bac6-b49ec98dce9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MobilenetV2.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"input_test\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc83888-67b8-4a2a-a8f2-b52787d2766e",
   "metadata": {},
   "source": [
    "Let's run DX-COM with this wrong JSON configuration. You will meet ERROR:\n",
    "\n",
    "> ConfigInputError: Config shape [1, 3, 224, 224] does not match preprocessed data shape [1, 224, 224, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ac0f7-0b8e-4dce-b9be-d31586657888",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57ef8e-0d99-425a-85c7-390480494f4c",
   "metadata": {},
   "source": [
    "#### 2.3. Add `transpose` & `expanDim` to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bbd86f-e2c9-4aef-8c4d-96ed3974caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"input_test\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cd446-8d2e-4c74-98ba-1e481eb57cfc",
   "metadata": {},
   "source": [
    "### 3. Compile with DX-Compiler and verify .dxnn\n",
    "Compile with dx_com to generate .dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2270e5a-6b04-457b-a7b1-33a3948515d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2907f72-5dc2-435b-9dc5-7b319a8bd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m MobilenetV2.dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30a3ea-3335-4809-9b65-e0ea0479db29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Compiling Object Detection Model (YOLOv9s)\n",
    "1. Export PyTorch â†’ ONNX\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133bb6f-807e-43ba-8fa9-1c55b5691280",
   "metadata": {},
   "source": [
    "### 1. Export PyTorch â†’ ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b15e4a-d1dd-4968-a046-e5217b12972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3dbaa-e9b2-48ea-aacf-b08f7f26124b",
   "metadata": {},
   "source": [
    "#### 1.1. Download `yolov9-s.pt` pytorch model to your local system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cfb8d-1f8e-4e70-8d2a-28dfaff0c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd yolov9 && pip install -r requirements.txt\n",
    "!pip install torch==2.5.1 torchvision==0.20.1\n",
    "!pip install onnx onnxsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae6a3d-d15c-4861-9794-dedd0ad4aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-s.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218e930-06a4-4be6-bf02-80f64c150f34",
   "metadata": {},
   "source": [
    "#### 1.2. Export the downloaded torch model to ONNX:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c0d0b-c1d9-4128-8c0a-afc36fbcf020",
   "metadata": {},
   "source": [
    "Download yolov9 git repo to use `export.py` for yolov9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581e7e5-8d01-4bea-b11b-a59d2de35968",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov9.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4799a-bd6a-4551-bded-724390318036",
   "metadata": {},
   "source": [
    "Export Pytorch based `yolov9-s` model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9d87a-9ce4-40c5-8722-758e356e817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd yolov9 && python3 export.py --weights ../yolov9-s.pt \\\n",
    "                                --img-size 640 640 \\\n",
    "                                --opset 12 \\\n",
    "                                --simplify \\\n",
    "                                --batch-size 1 \\\n",
    "                                --include onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee5d9f-7328-49a2-bfc5-77eef6ac9b44",
   "metadata": {},
   "source": [
    "Check if `yolov9-s.onnx` file is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d853b-16f6-4d36-8419-582d7ec034b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls | grep yolov9-s.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4186075-a2ec-4147-80a7-bac3ebe3608d",
   "metadata": {},
   "source": [
    "### 2. JSON configuration for Input/Pre-processing/Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9c6bc5-01a4-443b-9a0c-4e0ea048acd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting abc.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile yolov9-s.json\n",
    "{\n",
    "  \"inputs\": {\"images\": [1,3,640,640]},\n",
    "  \"calibration_num\": 100,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"div\": {\"x\": 255}},\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bd2bb-83b2-4646-80ec-f0aa528bbe61",
   "metadata": {},
   "source": [
    "### 3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56efd1b-4e30-436a-87ff-96f6e2aef06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m yolov9-s.onnx -c yolov9-s.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdea9f5-9c9a-42f5-a397-3b1d6a2be817",
   "metadata": {},
   "source": [
    "Check if **yolov9-s.dxnn** file is generated as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9074fe4-a5b0-44e9-91f7-af519106e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls | grep yolov9-s.dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f2f50a-5fe9-49af-9cb9-bac6fe82eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m yolov9-s.dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e1b99-96ca-412e-98bc-304e57c6c6bf",
   "metadata": {},
   "source": [
    "## Compiling ViT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca25b64-3bfd-4c60-86a7-84a3c240618a",
   "metadata": {},
   "source": [
    "https://github.com/mlfoundations/open_clip\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
