{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71513b35-9e60-4c49-b3ac-45c66d7f3933",
   "metadata": {},
   "source": [
    "# DEEPX Tutorial 05 - DX Compile workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38306cf-27f3-454b-9794-eb4ce4ee76a3",
   "metadata": {},
   "source": [
    "In Tutorial 5, you will practice compiling classification, object detection, and segmentation models using the DX-Compiler. We will also review the guide for troubleshooting problems during compilation.\n",
    "\n",
    "For more details, refer to the DX-COM user guide üëâ [Download](https://developer.deepx.ai/?files=MjUxNQ==)\n",
    "\n",
    ">This tutorial is based on dx-all-suite v2.0.0, released in September 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3d09f-4ccd-4589-8c7a-aa58cfd510fc",
   "metadata": {},
   "source": [
    "## Compiling Image Classification Model\n",
    "1. Export PyTorch ‚Üí ONNX\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cbee8-6cbc-4a78-8700-1cdc65076c4d",
   "metadata": {},
   "source": [
    "### 1. Export PyTorch ‚Üí ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b5de9-ab8a-466e-bfba-823f0fe96817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch & onnx\n",
    "!pip install --quiet torch torchvision onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87986a8c-af2b-43f5-8860-c63f0bc14ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639e9cb-7a79-422a-93e4-700ad8fcf472",
   "metadata": {},
   "source": [
    "Export Pytorch based MobileNetV2 model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e609d-ba12-45e5-8f40-f0d443165d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
    "model.eval()\n",
    "dummy = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "onnx_path = \"MobilenetV2.onnx\"\n",
    "torch.onnx.export(model, dummy, onnx_path, input_names=[\"input\"], output_names=[\"output\"], opset_version=13)\n",
    "print(\"‚úÖ Save ONNX:\", onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435e445-e87e-4706-ad8f-09d040477cb0",
   "metadata": {},
   "source": [
    "### 2. JSON configuration for Input/Pre-processing/Calibration\n",
    "Generate a configuration file for Input/Pre-processing/Calibration of MobilenetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe57234-a230-4925-be56-b3a410f26ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "# Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî ÏòàÏãú: ./dx_com/calibration_dataset\n",
    "cfg = {\n",
    "  \"inputs\": {\"input\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}}\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "with open(\"MobilenetV2.json\",\"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "print(\"‚úÖ JSON Ï†ÄÏû•: MobilenetV2.json\")\n",
    "os.system(\"sed -n '1,120p' MobilenetV2.json\")\n",
    "\n",
    "%cp MobilenetV2.json $root_path/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cd446-8d2e-4c74-98ba-1e481eb57cfc",
   "metadata": {},
   "source": [
    "### 3. Compile with DX-Compiler and verify .dxnn\n",
    "Compile with dx_com to generate .dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2270e5a-6b04-457b-a7b1-33a3948515d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206ab17-b328-4808-ad6a-99a82d4d21ee",
   "metadata": {},
   "source": [
    "### 4. Update JSON configuration to fix the following error:\n",
    "> ConfigInputError: Config shape [1, 3, 224, 224] does not match preprocessed data shape [1, 224, 224, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23164b73-5680-4a5d-ac7f-799247cc3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "# Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî ÏòàÏãú: ./dx_com/calibration_dataset\n",
    "cfg = {\n",
    "  \"inputs\": {\"input\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "with open(\"MobilenetV2.json\",\"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "print(\"‚úÖ JSON Ï†ÄÏû•: MobilenetV2.json\")\n",
    "os.system(\"sed -n '1,120p' MobilenetV2.json\")\n",
    "\n",
    "%cp MobilenetV2.json $root_path/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75b22b-bfbd-4f08-a830-c4cf64eb83a0",
   "metadata": {},
   "source": [
    "### 5. Re-compile with DX-Compiler and verify .dxnn\n",
    "Compile with dx_com to generate .dxnn and verify it with run_model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104394e-c7e3-4158-b291-b47a7adaf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./\n",
    "!ls | grep MobilenetV2.dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf9af1-c8ac-4524-835a-f9d5cb4ceed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m MobilenetV2.dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30a3ea-3335-4809-9b65-e0ea0479db29",
   "metadata": {},
   "source": [
    "## Compiling Object Detection Model\n",
    "1. Export PyTorch ‚Üí ONNX\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d93ca-05f7-4a07-90dd-4e4fadb42561",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx onnxscript ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15b15e4a-d1dd-4968-a046-e5217b12972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/max/Works/dx-tutorials/dx-all-suite/workspace/release/dx_com/dx_com_M1_v2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50012577-b287-486c-8e9c-64a307821513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-29 21:23:07--  https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-s.pt\n",
      "Resolving github.com (github.com)... 20.200.245.247\n",
      "Connecting to github.com (github.com)|20.200.245.247|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/759338070/5efccc50-06db-4a1f-bf78-db9fcce17d09?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-29T13%3A08%3A00Z&rscd=attachment%3B+filename%3Dyolov9-s.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-29T12%3A07%3A18Z&ske=2025-10-29T13%3A08%3A00Z&sks=b&skv=2018-11-09&sig=5CAPFguDYI8mi3UPAcFW4ReYHYr5jqkdsen4mO6CTII%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTc0MjM4NywibmJmIjoxNzYxNzQwNTg3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.GBhAAF9QCtEtFfxBog86Q6wKsSPbmohm18OjDcLYh0c&response-content-disposition=attachment%3B%20filename%3Dyolov9-s.pt&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-10-29 21:23:08--  https://release-assets.githubusercontent.com/github-production-release-asset/759338070/5efccc50-06db-4a1f-bf78-db9fcce17d09?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-29T13%3A08%3A00Z&rscd=attachment%3B+filename%3Dyolov9-s.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-29T12%3A07%3A18Z&ske=2025-10-29T13%3A08%3A00Z&sks=b&skv=2018-11-09&sig=5CAPFguDYI8mi3UPAcFW4ReYHYr5jqkdsen4mO6CTII%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTc0MjM4NywibmJmIjoxNzYxNzQwNTg3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.GBhAAF9QCtEtFfxBog86Q6wKsSPbmohm18OjDcLYh0c&response-content-disposition=attachment%3B%20filename%3Dyolov9-s.pt&response-content-type=application%2Foctet-stream\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20423912 (19M) [application/octet-stream]\n",
      "Saving to: ‚Äòyolov9-s.pt‚Äô\n",
      "\n",
      "yolov9-s.pt         100%[===================>]  19.48M  9.10MB/s    in 2.1s    \n",
      "\n",
      "2025-10-29 21:23:10 (9.10 MB/s) - ‚Äòyolov9-s.pt‚Äô saved [20423912/20423912]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c41bee4-2be8-4493-9227-577bdbdb7b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.213 üöÄ Python-3.12.3 torch-2.9.0+cu128 CPU (AMD Ryzen 5 9600X 6-Core Processor)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 658 layers, 9659168 parameters, 0 gradients, 39.1 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov9-s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((), ()) (19.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1029 21:23:22.850000 58963 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 13 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 13).\n",
      "Failed to convert the model to the target version 13 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/max/Works/dx-tutorials/.venv/lib/python3.12/site-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max/Works/dx-tutorials/.venv/lib/python3.12/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/home/max/Works/dx-tutorials/.venv/lib/python3.12/site-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max/Works/dx-tutorials/.venv/lib/python3.12/site-packages/onnx/version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: /github/workspace/onnx/version_converter/BaseConverter.h:65: adapter_lookup: Assertion `false` failed: No Adapter To Version $17 for Resize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.72...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 3.5s, saved as 'yolov9-s.onnx' (37.1 MB)\n",
      "\n",
      "Export complete (4.2s)\n",
      "Results saved to \u001b[1m/home/max/Works/dx-tutorials/dx-all-suite/workspace/release/dx_com/dx_com_M1_v2.0.0\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov9-s.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov9-s.onnx imgsz=640 data=None  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov9-s.onnx'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov9-s.pt\") \n",
    "\n",
    "# Export to ONNX, fixed input size 1x3x640x640, simplified graph\n",
    "model.export(\n",
    "    format=\"onnx\",\n",
    "    imgsz=640,          # or (640, 640)\n",
    "    opset=13,           # 12‚Äì13 is the most compatible range\n",
    "    simplify=True,\n",
    "    dynamic=False       # fixed shape (batch=1, H=W=640)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd46e044-6f12-45e9-bf2c-033d66e6cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp yolov9-s.onnx ~/yolov9-s-non-simple.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "035b8d5e-202a-4568-a36f-eaf51cd4ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSON Ï†ÄÏû•: yolov9-s.json\n",
      "{\n",
      "  \"inputs\": {\n",
      "    \"images\": [\n",
      "      1,\n",
      "      3,\n",
      "      640,\n",
      "      640\n",
      "    ]\n",
      "  },\n",
      "  \"calibration_num\": 100,\n",
      "  \"calibration_method\": \"ema\",\n",
      "  \"default_loader\": {\n",
      "    \"dataset_path\": \"./calibration_dataset\",\n",
      "    \"file_extensions\": [\n",
      "      \"jpeg\",\n",
      "      \"jpg\",\n",
      "      \"png\",\n",
      "      \"JPEG\"\n",
      "    ],\n",
      "    \"preprocessings\": [\n",
      "      {\n",
      "        \"resize\": {\n",
      "          \"mode\": \"pad\",\n",
      "          \"size\": 640,\n",
      "          \"pad_location\": \"edge\",\n",
      "          \"pad_value\": [\n",
      "            114,\n",
      "            114,\n",
      "            114\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"div\": {\n",
      "          \"x\": 255\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"convertColor\": {\n",
      "          \"form\": \"BGR2RGB\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"transpose\": {\n",
      "          \"axis\": [\n",
      "            2,\n",
      "            0,\n",
      "            1\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"expandDim\": {\n",
      "          \"axis\": 0\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "# Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî ÏòàÏãú: ./dx_com/calibration_dataset\n",
    "cfg = {\n",
    "  \"inputs\": {    \"images\": [      1,      3,      640,      640    ]  },\n",
    "  \"calibration_num\": 100,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [      \"jpeg\",      \"jpg\",      \"png\",      \"JPEG\"    ],\n",
    "    \"preprocessings\": [\n",
    "      {\n",
    "        \"resize\": {          \"mode\": \"pad\",          \"size\": 640,          \"pad_location\": \"edge\",\n",
    "          \"pad_value\": [            114,            114,            114          ]        }\n",
    "      },\n",
    "      {\n",
    "        \"div\": {          \"x\": 255        }\n",
    "      },\n",
    "      {\n",
    "        \"convertColor\": {          \"form\": \"BGR2RGB\"        }\n",
    "      },\n",
    "      {\n",
    "        \"transpose\": {          \"axis\": [            2,            0,            1          ]        }\n",
    "      },\n",
    "      {        \"expandDim\": {          \"axis\": 0        }      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(\"yolov9-s.json\",\"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "print(\"‚úÖ JSON Ï†ÄÏû•: yolov9-s.json\")\n",
    "os.system(\"sed -n '1,120p' yolov9-s.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a56efd1b-4e30-436a-87ff-96f6e2aef06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Model :  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 0.8/1.0 [00:19<00:04, 22.03s/model ]Nuitka: A segmentation fault has occurred. This is highly unusual and can\n",
      "have multiple reasons. Please check https://nuitka.net/info/segfault.html\n",
      "for solutions.\n"
     ]
    }
   ],
   "source": [
    "!./dx_com/dx_com -m yolov9-s.onnx -c yolov9-s.json -o ./"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
