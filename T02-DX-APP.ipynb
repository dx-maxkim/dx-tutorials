{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528ffbf3-26f1-4b0b-b49e-651397413a51",
   "metadata": {},
   "source": [
    "# DEEPX Tutorial 02 - Usage of DX_APP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d745a2c-590f-466c-8d99-dac51463d1a7",
   "metadata": {},
   "source": [
    "In this second tutorial, we will introduce DX_APP and learn how to utilize a converted DXNN model in an AI application. Additionally, we will cover how to use a USB webcam as an input source.\n",
    "\n",
    "This tutorial is based on dx-all-suite v2.0.0, released in September 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c593151-97c1-49c9-ac7b-d9149c7e58ba",
   "metadata": {},
   "source": [
    "## What is DX_APP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8fc6e5-dd5e-4014-a9bd-1aacde11dea1",
   "metadata": {},
   "source": [
    "**DX-APP** is a sample application that demonstrates how to run compiled models on actual DEEPX NPU\n",
    "using DX-RT. It includes ready-to-use code for common vision tasks such as object detection, face\n",
    "recognition, and image classification. DX-APP helps developers quickly set up the runtime environment\n",
    "and serves as a template for building and customizing their own AI applications.\n",
    "\n",
    "For more details, download DX_APP User Guide from ðŸ‘‰ [here](https://developer.deepx.ai/?files=MjUxOA==)!\n",
    "\n",
    "Let's see the file structure of DX_APP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae84aa2-63e1-4852-a0f7-c421f4d5157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/donggyun/git/dx-tutorials/dx-all-suite\n"
     ]
    }
   ],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874e805-7b93-4b69-a82e-9f5c60e2788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 1 dx-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6631c-caa4-4c4f-b218-d21a0d42dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 1 dx-runtime/dx_app\n",
    "#!tree -L 1 dx-runtime/dx_app/bin\n",
    "#!tree -L 1 dx-runtime/dx_app/demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec96cc-5fe7-4a93-a39e-709db97d0465",
   "metadata": {},
   "source": [
    "**DX-APP** demos are optimized to showcase pre-compiled models on DEEPX NPUs with minimal setup.\n",
    "Each demo represents a common AI task and can be executed using images, videos, or live camera\n",
    "input.\n",
    "\n",
    "**Classification**\n",
    "- Executes classification models with image inputs (e.g., 224x224 ).\n",
    "- Outputs the Top-1 predicted class.\n",
    "- Example: example/run_classifier/imagenet_example.json\n",
    "\n",
    "**Object Detection**\n",
    "- For image input, outputs result.jpg and prints detected objects to the terminal.\n",
    "- For video input, displays bounding boxes on the output video.\n",
    "\n",
    "**Pose Estimation**\n",
    "- Detects people and estimates keypoints (joints) using image, video, or camera input.\n",
    "- The output includes both bounding boxes and joint coordinates rendered on screen.\n",
    "\n",
    "**Segmentation**\n",
    "- For image input, saves results to result.jpg and prints info to the terminal.\n",
    "- For video input, displays output with both detection boxes and segmentation masks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1addf3af-3464-4c65-bd6b-8c9bce945acc",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abf2e7-8ca9-42bc-81fd-2af1cb482a46",
   "metadata": {},
   "source": [
    "1. Move to `dx_app` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6584369-c62a-4604-8021-47e3e42d8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-runtime/dx_app\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-runtime/dx_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf26c3-cd85-484c-bc4c-ef1a53f3b54a",
   "metadata": {},
   "source": [
    "2. Download required models and sample videos by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39177926-c29f-4701-9af1-a2f80525bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assets (models + videos) are downloaded and placed in the assets/ directory.\n",
    "!./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feedff76-7e3e-4407-8fd4-45ce6f44bafc",
   "metadata": {},
   "source": [
    "3. Verify that both models and videos are downloaded as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43627e-0705-4759-b60d-e40746d174cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI models converted to DXNN format\n",
    "!tree assets/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3b1d9-3c92-42d8-98b8-ad2018fc8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video files for demo inputs\n",
    "!tree assets/videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48695b-d00c-47cc-9129-fbf41b63af73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## USB Webcam Basics (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4e9b4-602c-4a35-8b4c-b54986382017",
   "metadata": {},
   "source": [
    "This hands-on notebook shows how to:\n",
    "- Discover USB webcams and inspect capabilities with **V4L2** (`v4l2-ctl`).\n",
    "- Configure formats (e.g., **MJPEG** or **YUYV**), resolution, FPS, and camera controls (exposure, focus, WB).\n",
    "- Capture images and video with **OpenCV** (both windowed & headless modes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e232d1-b618-4f7f-a80c-45c759ad0f88",
   "metadata": {},
   "source": [
    "### 0. Prerequisites (run once):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b75ed-6d72-4146-9f4e-384c72e0da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y v4l-utils\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c24517-d7da-43fc-bbfd-11cc69ef4997",
   "metadata": {},
   "source": [
    "### 1. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63484d94-0f23-4202-88a7-13b8ff462c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform, subprocess, shutil, os, time, re, json, glob, pathlib\n",
    "import cv2\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"OpenCV:\", cv2.__version__)\n",
    "\n",
    "# Check v4l2-ctl availability\n",
    "v4l2_path = shutil.which(\"v4l2-ctl\")\n",
    "print(\"v4l2-ctl:\", v4l2_path if v4l2_path else \"NOT FOUND - please `sudo apt install v4l-utils`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fa430-61df-4e86-98ff-4f30b9372553",
   "metadata": {},
   "source": [
    "### 2. Discover Video Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f000c6-8760-4033-9540-92bf2ffb57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List /dev/video* nodes\n",
    "video_nodes = sorted(glob.glob(\"/dev/video*\"))\n",
    "print(\"Detected video nodes:\", video_nodes)\n",
    "\n",
    "# v4l2-ctl --list-devices gives a nice mapping (device -> /dev/videoX)\n",
    "if v4l2_path:\n",
    "    print(\"\\n== v4l2-ctl --list-devices ==\")\n",
    "    print(subprocess.run([\"v4l2-ctl\", \"--list-devices\"], capture_output=True, text=True).stdout)\n",
    "else:\n",
    "    print(\"v4l2-ctl not available; skipping device listing via v4l2-ctl.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b2c49-6dbc-46f5-80f0-31cba4b210ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /dev/video*\n",
    "#!v4l2-ctl --list-devices\n",
    "#!cat /sys/class/video4linux/video*/name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173bfc3-123a-4b42-9e7b-e56c6eb2a2c0",
   "metadata": {},
   "source": [
    "### 3. Choose Your Webcam Device\n",
    "Set `DEVICE` to the `/dev/videoX` node of your webcam. If you're unsure, pick the first one that shows UVC capabilities in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141687f-b71d-41c1-8031-674c44b4a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your webcam node if needed\n",
    "DEVICE = \"/dev/video0\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a857d-f0d7-4e2b-b1e9-13fbf7fe1a36",
   "metadata": {},
   "source": [
    "### 4. Inspect Capabilities, Formats, and Frame Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d172a0-6509-48fb-8acd-5a854c6eeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DEVICE):\n",
    "    raise FileNotFoundError(f\"{DEVICE} not found. Update DEVICE to a valid /dev/videoX.\")\n",
    "\n",
    "if v4l2_path:\n",
    "    print(\"== v4l2-ctl --device --all ==\")\n",
    "    print(subprocess.run([\"v4l2-ctl\", f\"--device={DEVICE}\", \"--all\"], capture_output=True, text=True).stdout)\n",
    "\n",
    "    print(\"\\n== v4l2-ctl --device --list-formats-ext ==\")\n",
    "    print(subprocess.run([\"v4l2-ctl\", f\"--device={DEVICE}\", \"--list-formats-ext\"], capture_output=True, text=True).stdout)\n",
    "else:\n",
    "    print(\"v4l2-ctl not available; cannot show capabilities/formats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17893b-cf8c-4d59-a2a5-e6cc0a971add",
   "metadata": {},
   "outputs": [],
   "source": [
    "!v4l2-ctl --device {DEVICE} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1fea3a-7fe8-4e72-aeae-3aeca90f1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!v4l2-ctl --device {DEVICE} --list-formats-ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc7cce-2c6a-4969-a2f5-5116d7c99854",
   "metadata": {},
   "source": [
    "### 5. (Optional) Configure Format & FPS via V4L2\n",
    "\n",
    "Two common pixel formats:\n",
    "- **MJPG** (Motion JPEG): lower USB bandwidth, lighter CPU decode than raw â†’ often best for 1080p+ over USB.\n",
    "- **YUYV** (YUYV 4:2:2): raw frames, higher bandwidth but low latency and no compression artifacts.\n",
    "\n",
    "> We'll try setting **1920x1080 @ 30fps** with **MJPG**. Adjust if unsupported by your camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83cebb-72ab-4185-a17b-75a004a9ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFERRED_WIDTH, PREFERRED_HEIGHT, PREFERRED_FPS = 1920, 1080, 30\n",
    "PREFERRED_FOURCC = \"MJPG\"  # or \"YUYV\"\n",
    "\n",
    "if v4l2_path:\n",
    "    print(\"Setting format via v4l2-ctl ...\")\n",
    "    cmds = [\n",
    "        [\"v4l2-ctl\", f\"--device={DEVICE}\", f\"--set-fmt-video=width={PREFERRED_WIDTH},height={PREFERRED_HEIGHT},pixelformat={PREFERRED_FOURCC}\"],\n",
    "        [\"v4l2-ctl\", f\"--device={DEVICE}\", f\"--set-parm={PREFERRED_FPS}\"]\n",
    "    ]\n",
    "    for c in cmds:\n",
    "        res = subprocess.run(c, capture_output=True, text=True)\n",
    "        print(\"$\", \" \".join(c))\n",
    "        if res.stderr.strip():\n",
    "            print(\"stderr:\", res.stderr.strip())\n",
    "        if res.stdout.strip():\n",
    "            print(res.stdout.strip())\n",
    "else:\n",
    "    print(\"v4l2-ctl not available; skip format set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc37e5-92a6-4dd2-92e9-75d623c880e9",
   "metadata": {},
   "source": [
    "### 6. OpenCV Capture Basics\n",
    "\n",
    "We'll show two patterns:\n",
    "\n",
    "- **Headless (Notebook)**: display a few frames inline (no GUI windows).\n",
    "- **Windowed (Desktop)**: show a live window. Use this on a local desktop with a display server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cca55-f3cd-486c-b373-30ae3ae7a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_capture(dev=\"/dev/video0\", width=1280, height=720, fps=30, fourcc=\"MJPG\"):\n",
    "    cap = cv2.VideoCapture(dev, cv2.CAP_V4L2)  # prefer V4L2 backend on Linux\n",
    "    if fourcc:\n",
    "        # set FOURCC before size/fps for reliability\n",
    "        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*fourcc))\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "    cap.set(cv2.CAP_PROP_FPS, fps)\n",
    "    # Some drivers report after opening; query back\n",
    "    actual = {\n",
    "        \"fourcc\": int(cap.get(cv2.CAP_PROP_FOURCC)),\n",
    "        \"width\": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        \"height\": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "        \"fps\": cap.get(cv2.CAP_PROP_FPS),\n",
    "        \"backend\": int(cap.get(cv2.CAP_PROP_BACKEND))\n",
    "    }\n",
    "    return cap, actual\n",
    "\n",
    "cap, actual = open_capture(DEVICE, PREFERRED_WIDTH, PREFERRED_HEIGHT, PREFERRED_FPS, PREFERRED_FOURCC)\n",
    "print(\"Actual settings:\", actual)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Failed to open the camera. Check permissions and device node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b1803-5541-4c67-80b1-564f52c2e2bb",
   "metadata": {},
   "source": [
    "#### 6.1 Headless Preview (Inline Frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2f72e-af4a-42ee-98d7-80fdabaa7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Capture 2 snapshots\n",
    "n_frames = 2\n",
    "imgs = []\n",
    "for i in range(n_frames):\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        print(\"Failed to read frame\")\n",
    "        break\n",
    "    # Optional: convert color if needed (OpenCV default is BGR)\n",
    "    # display inline\n",
    "    _, buf = cv2.imencode(\".jpg\", frame)\n",
    "    display(Markdown(f\"**Frame {i+1}**\"))\n",
    "    display(widgets.Image(value=buf.tobytes(), format='jpg', width=512))\n",
    "\n",
    "# Keep cap open for subsequent cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1c34c-a956-4b43-b31f-54b4201f57b4",
   "metadata": {},
   "source": [
    "#### 6.2 Windowed Live View\n",
    "\n",
    "> Run this only on a local desktop with a display (won't work on headless servers). Press **q** to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d058b-5c11-4f73-a6bd-81c33799c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, time\n",
    "win = \"Live\"\n",
    "#cv2.namedWindow(win, cv2.WINDOW_NORMAL)\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "    cv2.imshow(win, frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110dc6f-3e7a-4fba-800d-afc948d62f78",
   "metadata": {},
   "source": [
    "### 7. Handling YUYV (Raw 4:2:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9ff0c-6bb9-46f6-a23d-6825dfe2268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If MJPG is unavailable or you prefer raw frames, try YUYV.\n",
    "# We'll reopen with YUYV to demonstrate conversion.\n",
    "\n",
    "cap.release()\n",
    "cap, actual = open_capture(DEVICE, 640, 480, 30, \"YUYV\")\n",
    "print(\"Reopened with YUYV. Actual:\", actual)\n",
    "\n",
    "ok, frame = cap.read()\n",
    "if not ok:\n",
    "    print(\"Failed to read YUYV frame; your camera/driver may not support raw at this size/fps.\")\n",
    "else:\n",
    "    # Some backends already convert to BGR; if you get a single channel or strange shape, use cvtColor:\n",
    "    # Example: yuyv_bgr = cv2.cvtColor(frame, cv2.COLOR_YUV2BGR_YUY2)\n",
    "    # For demo, we will just show whatever we get:\n",
    "    _, buf = cv2.imencode(\".jpg\", frame)\n",
    "    display(Markdown(f\"**Frame {i+1}**\"))\n",
    "    display(widgets.Image(value=buf.tobytes(), format='jpg', width=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b336d-4a65-436a-8024-5094e7392138",
   "metadata": {},
   "source": [
    "## Run Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f5be6-8a4d-49ad-9a60-a26ee425696e",
   "metadata": {},
   "source": [
    "**DX_APP** is a set of ready-made demo applications that show how to run compiled models on DEEPX NPUs, including classification, detection, segmentation, pose estimation, and many others.\n",
    "\n",
    "Users can learn and build applications by exploring the diverse DX-APP demos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf03bd-4b9a-4a95-baaf-62100bda6a94",
   "metadata": {},
   "source": [
    "### Classification\n",
    "- **bin/classification** is the DX_APP demo binary that runs a pre-trained image classification model and outputs the top predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eba146-79f6-4d89-be95-92c65ff36636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls bin\n",
    "!./bin/classification -h\n",
    "#!./bin/classification -m assets/models/EfficientNetB0_4.dxnn -i sample/ILSVRC2012/1.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e228439-78cd-4b7e-83dd-23bffe2def95",
   "metadata": {},
   "source": [
    "### Object Detection\n",
    "- **bin/yolo** is the DX_APP demo binary that runs YOLO-based object detection models to detect objects in images, video or camera.\n",
    "- **bin/yolo_multi** is the extended version of bin/yolo to support yolo-based multi use-cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198471e-c45b-4edd-9693-409719c4f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./bin/yolo -h\n",
    "#!./bin/yolo -m assets/models/YOLOV5S_3.dxnn -i sample/face_sample.jpg -p 1\n",
    "#!eog result.jpg\n",
    "#!./bin/yolo -m assets/models/YOLOV5S_3.dxnn -v assets/videos/boat.mp4 -p 1\n",
    "#!./bin/yolo -m assets/models/YOLOV5S_PPU.dxnn -v assets/videos/boat.mp4 -p 11\n",
    "#!./bin/yolo -m assets/models/YOLOV5S_3.dxnn -p 1 -c\n",
    "#!./bin/yolo -m assets/models/YOLOV5S_3.dxnn -p 1 -c --camera_path /dev/video0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbae31-1e12-47f7-9398-756583f57ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./bin/yolo_multi -h\n",
    "#!cat example/yolo_multi/yolo_multi_demo.json\n",
    "#!./bin/yolo_multi -c example/yolo_multi/yolo_multi_demo.json\n",
    "#!./bin/yolo_multi -c example/yolo_multi/ppu_yolo_multi_demo.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322638df-c151-40e1-960f-9262f721b3dc",
   "metadata": {},
   "source": [
    "### Pose Estimation\n",
    "- **bin/pose** is the DX_APP demo binary that runs a pose estimation model to detect people and estimate keypoints in images, video or camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07558a9-8a49-4daa-bc91-e4d4a45a156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./bin/pose -h\n",
    "#!./bin/pose -m assets/models/YOLOV5Pose640_1.dxnn -i sample/7.jpg -p 0\n",
    "#!./bin/pose -m assets/models/YOLOV5Pose640_1.dxnn -v assets/videos/dance-solo.mov -p 0\n",
    "#!./bin/pose -m assets/models/YOLOV5Pose_PPU.dxnn -v assets/videos/dance-solo.mov -p 1\n",
    "#!./bin/pose -m assets/models/YOLOV5Pose640_1.dxnn -c -p 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed097f7-bf23-4bf7-95b3-17ee8e288cff",
   "metadata": {},
   "source": [
    "### Segmentation\n",
    "- **bin/segmentation** is the DX_APP demo binary that runs a semantic segmentation model to produce pixel-wise class labels for an image, video or camera.\n",
    "- **bin/od_segmentation** is to run a combined segmentation and detection demo using two compiled DXNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27461eca-5e94-411e-828d-6bd585ec70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./bin/segmentation -h\n",
    "#!./bin/segmentation -m assets/models/DeepLabV3PlusMobileNetV2_2.dxnn -i sample/8.jpg\n",
    "#!./bin/segmentation -m assets/models/DeepLabV3PlusMobileNetV2_2.dxnn -v assets/videos/blackbox-city-road.mp4\n",
    "#!./bin/segmentation -m assets/models/DeepLabV3PlusMobileNetV2_2.dxnn -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f82e4-edcf-4e13-8500-84729fceed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./bin/od_segmentation -h\n",
    "#!./bin/od_segmentation -m0 assets/models/YoloV7.dxnn -p0 3 -m1 assets/models/DeepLabV3PlusMobileNetV2_2.dxnn -i sample/8.jpg\n",
    "#!./bin/od_segmentation -m0 assets/models/YoloV7.dxnn -p0 3 -m1 assets/models/DeepLabV3PlusMobileNetV2_2.dxnn -v assets/videos/blackbox-city-road2.mov\n",
    "#!./bin/od_segmentation -m0 assets/models/YoloV7.dxnn -p0 3 -m1 assets/models/DeepLabV3PlusMobileNetV2_2.dxnn -c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
